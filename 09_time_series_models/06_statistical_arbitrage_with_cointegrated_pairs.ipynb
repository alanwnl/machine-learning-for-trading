{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical arbitrage with Cointegration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairs Trading & Statistical Arbitrage\n",
    "\n",
    "Statistical arbitrage refers to strategies that employ some statistical model or method to take\n",
    "advantage of what appears to be relative mispricing of assets, while maintaining a level of\n",
    "market neutrality.\n",
    "\n",
    "Pairs trading is a conceptually straightforward strategy that has been employed by algorithmic traders since at least the mid-eighties ([Gatev, Goetzmann, and Rouwenhorst 2006](http://www-stat.wharton.upenn.edu/~steele/Courses/434/434Context/PairsTrading/PairsTradingGGR.pdf)). The goal is to find two assets whose prices have historically moved together, track the spread (the difference between their prices), and, once the spread widens, buy the\n",
    "loser that has dropped below the common trend and short the winner. If the relationship persists, the long and/or the short leg will deliver profits as prices converge and the positions are closed.\n",
    "\n",
    "This approach extends to a multivariate context by forming baskets from multiple securities and trading one asset against a basket of two baskets against each other.\n",
    "\n",
    "## Pairs Trading in Practice\n",
    "\n",
    "In practice, the strategy requires two steps:\n",
    "\n",
    "1. **Formation phase**: Identify securities that have a long-term mean-reverting relationship. Ideally, the spread should have a high variance to allow for frequent profitable trades while reliably reverting to the common trend.\n",
    "2. **Trading phase**: Trigger entry and exit trading rules as price movements cause thespread to diverge and converge.\n",
    "\n",
    "Several approaches to the formation and trading phases have emerged from increasingly active research in this area, across multiple asset classes, over the last several years. The book outlines the key differences between them; the notebook dives into an example application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:42:47.349889Z",
     "start_time": "2020-06-19T02:42:47.347632Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:42:47.908593Z",
     "start_time": "2020-06-19T02:42:47.351191Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pykalman import KalmanFilter\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:42:47.912091Z",
     "start_time": "2020-06-19T02:42:47.909755Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:42:47.933578Z",
     "start_time": "2020-06-19T02:42:47.927007Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return f'{h:>02.0f}:{m:>02.0f}:{s:>02.0f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Johansen Test Critical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:42:47.943279Z",
     "start_time": "2020-06-19T02:42:47.934611Z"
    }
   },
   "outputs": [],
   "source": [
    "critical_values = {0: {.9: 13.4294, .95: 15.4943, .99: 19.9349},\n",
    "                   1: {.9: 2.7055, .95: 3.8415, .99: 6.6349}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:42:47.951303Z",
     "start_time": "2020-06-19T02:42:47.944195Z"
    }
   },
   "outputs": [],
   "source": [
    "trace0_cv = critical_values[0][.95] # critical value for 0 cointegration relationships\n",
    "trace1_cv = critical_values[1][.95] # critical value for 1 cointegration relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:03:36.116641Z",
     "start_time": "2020-06-19T13:03:36.109897Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = Path('..', 'data') \n",
    "STORE = DATA_PATH / 'assets.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get backtest prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine OHLCV prices for relevant stock and ETF tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:09:37.773341Z",
     "start_time": "2020-06-19T13:09:37.761245Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_backtest_prices():\n",
    "    with pd.HDFStore('data.h5') as store:\n",
    "        tickers = store['tickers']\n",
    "\n",
    "    with pd.HDFStore(STORE) as store:\n",
    "        prices = (pd.concat([\n",
    "            store['stooq/us/nyse/stocks/prices'],\n",
    "            store['stooq/us/nyse/etfs/prices'],\n",
    "            store['stooq/us/nasdaq/etfs/prices'],\n",
    "            store['stooq/us/nasdaq/stocks/prices']])\n",
    "                  .sort_index()\n",
    "                  .loc[idx[tickers.index, '2016':'2019'], :])\n",
    "    print(prices.info(show_counts=True))\n",
    "    prices.to_hdf('backtest.h5', 'prices')\n",
    "    tickers.to_hdf('backtest.h5', 'tickers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:10:04.571934Z",
     "start_time": "2020-06-19T13:09:38.060713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 305824 entries, ('AA.US', Timestamp('2016-01-04 00:00:00')) to ('WYNN.US', Timestamp('2019-12-31 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   open    305824 non-null  float64\n",
      " 1   high    305824 non-null  float64\n",
      " 2   low     305824 non-null  float64\n",
      " 3   close   305824 non-null  float64\n",
      " 4   volume  305824 non-null  float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 13.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "get_backtest_prices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:42:48.043300Z",
     "start_time": "2020-06-19T02:42:47.984127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1258 entries, 2015-01-02 to 2019-12-31\n",
      "Columns: 172 entries, AAPL.US to AEP.US\n",
      "dtypes: float64(172)\n",
      "memory usage: 1.7 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>AAPL.US</th>\n",
       "      <th>AMZN.US</th>\n",
       "      <th>MSFT.US</th>\n",
       "      <th>BAC.US</th>\n",
       "      <th>GOOGL.US</th>\n",
       "      <th>NFLX.US</th>\n",
       "      <th>C.US</th>\n",
       "      <th>JPM.US</th>\n",
       "      <th>XOM.US</th>\n",
       "      <th>INTC.US</th>\n",
       "      <th>...</th>\n",
       "      <th>CLF.US</th>\n",
       "      <th>FITB.US</th>\n",
       "      <th>GEN.US</th>\n",
       "      <th>GAP.US</th>\n",
       "      <th>ADSK.US</th>\n",
       "      <th>FSLR.US</th>\n",
       "      <th>ADI.US</th>\n",
       "      <th>PCG.US</th>\n",
       "      <th>NVS.US</th>\n",
       "      <th>AEP.US</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>24.3203</td>\n",
       "      <td>15.4260</td>\n",
       "      <td>40.1536</td>\n",
       "      <td>15.3668</td>\n",
       "      <td>26.3819</td>\n",
       "      <td>49.849</td>\n",
       "      <td>45.0718</td>\n",
       "      <td>50.3059</td>\n",
       "      <td>63.7264</td>\n",
       "      <td>28.9303</td>\n",
       "      <td>...</td>\n",
       "      <td>6.6967</td>\n",
       "      <td>14.5194</td>\n",
       "      <td>10.27600</td>\n",
       "      <td>31.2373</td>\n",
       "      <td>59.53</td>\n",
       "      <td>44.545</td>\n",
       "      <td>46.7245</td>\n",
       "      <td>48.772</td>\n",
       "      <td>73.049</td>\n",
       "      <td>46.3700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>23.6361</td>\n",
       "      <td>15.1095</td>\n",
       "      <td>39.7826</td>\n",
       "      <td>14.9238</td>\n",
       "      <td>25.8791</td>\n",
       "      <td>47.311</td>\n",
       "      <td>43.6485</td>\n",
       "      <td>48.7429</td>\n",
       "      <td>61.9827</td>\n",
       "      <td>28.6040</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2108</td>\n",
       "      <td>14.0833</td>\n",
       "      <td>10.13220</td>\n",
       "      <td>31.4391</td>\n",
       "      <td>58.66</td>\n",
       "      <td>41.830</td>\n",
       "      <td>45.8705</td>\n",
       "      <td>49.594</td>\n",
       "      <td>73.210</td>\n",
       "      <td>45.6898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>23.6381</td>\n",
       "      <td>14.7645</td>\n",
       "      <td>39.1972</td>\n",
       "      <td>14.4711</td>\n",
       "      <td>25.2406</td>\n",
       "      <td>46.501</td>\n",
       "      <td>42.1145</td>\n",
       "      <td>47.4804</td>\n",
       "      <td>61.6540</td>\n",
       "      <td>28.0687</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0109</td>\n",
       "      <td>13.5169</td>\n",
       "      <td>9.97376</td>\n",
       "      <td>31.0230</td>\n",
       "      <td>57.50</td>\n",
       "      <td>40.860</td>\n",
       "      <td>44.7955</td>\n",
       "      <td>49.519</td>\n",
       "      <td>72.591</td>\n",
       "      <td>45.9386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>23.9734</td>\n",
       "      <td>14.9210</td>\n",
       "      <td>39.6975</td>\n",
       "      <td>14.5438</td>\n",
       "      <td>25.1663</td>\n",
       "      <td>46.743</td>\n",
       "      <td>42.5031</td>\n",
       "      <td>47.5512</td>\n",
       "      <td>62.2793</td>\n",
       "      <td>28.6587</td>\n",
       "      <td>...</td>\n",
       "      <td>6.5157</td>\n",
       "      <td>13.6660</td>\n",
       "      <td>10.06520</td>\n",
       "      <td>32.5231</td>\n",
       "      <td>57.38</td>\n",
       "      <td>41.750</td>\n",
       "      <td>45.2680</td>\n",
       "      <td>49.913</td>\n",
       "      <td>72.907</td>\n",
       "      <td>46.5922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>24.8927</td>\n",
       "      <td>15.0230</td>\n",
       "      <td>40.8676</td>\n",
       "      <td>14.8413</td>\n",
       "      <td>25.2540</td>\n",
       "      <td>47.779</td>\n",
       "      <td>43.1443</td>\n",
       "      <td>48.6122</td>\n",
       "      <td>63.3131</td>\n",
       "      <td>29.1950</td>\n",
       "      <td>...</td>\n",
       "      <td>6.9825</td>\n",
       "      <td>13.9755</td>\n",
       "      <td>10.24930</td>\n",
       "      <td>32.1495</td>\n",
       "      <td>58.80</td>\n",
       "      <td>43.630</td>\n",
       "      <td>46.0677</td>\n",
       "      <td>50.543</td>\n",
       "      <td>75.442</td>\n",
       "      <td>46.9913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>68.8225</td>\n",
       "      <td>89.4605</td>\n",
       "      <td>150.5140</td>\n",
       "      <td>32.8869</td>\n",
       "      <td>66.9786</td>\n",
       "      <td>333.200</td>\n",
       "      <td>70.4716</td>\n",
       "      <td>125.5370</td>\n",
       "      <td>58.7539</td>\n",
       "      <td>54.3557</td>\n",
       "      <td>...</td>\n",
       "      <td>8.2083</td>\n",
       "      <td>25.3039</td>\n",
       "      <td>13.91340</td>\n",
       "      <td>15.6452</td>\n",
       "      <td>183.91</td>\n",
       "      <td>57.980</td>\n",
       "      <td>113.2370</td>\n",
       "      <td>10.950</td>\n",
       "      <td>87.976</td>\n",
       "      <td>84.8095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>70.1874</td>\n",
       "      <td>93.4385</td>\n",
       "      <td>151.7760</td>\n",
       "      <td>33.1679</td>\n",
       "      <td>67.8774</td>\n",
       "      <td>332.630</td>\n",
       "      <td>71.5816</td>\n",
       "      <td>126.8730</td>\n",
       "      <td>58.8474</td>\n",
       "      <td>54.7318</td>\n",
       "      <td>...</td>\n",
       "      <td>8.1692</td>\n",
       "      <td>25.4341</td>\n",
       "      <td>13.78850</td>\n",
       "      <td>15.9020</td>\n",
       "      <td>184.24</td>\n",
       "      <td>58.660</td>\n",
       "      <td>113.2470</td>\n",
       "      <td>10.860</td>\n",
       "      <td>87.966</td>\n",
       "      <td>84.8459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>70.1597</td>\n",
       "      <td>93.4900</td>\n",
       "      <td>152.0480</td>\n",
       "      <td>33.0077</td>\n",
       "      <td>67.4874</td>\n",
       "      <td>329.090</td>\n",
       "      <td>71.4387</td>\n",
       "      <td>126.9610</td>\n",
       "      <td>58.6451</td>\n",
       "      <td>54.9702</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0324</td>\n",
       "      <td>25.2543</td>\n",
       "      <td>13.87010</td>\n",
       "      <td>15.8131</td>\n",
       "      <td>185.38</td>\n",
       "      <td>56.410</td>\n",
       "      <td>112.9800</td>\n",
       "      <td>10.440</td>\n",
       "      <td>88.486</td>\n",
       "      <td>85.1279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>70.5778</td>\n",
       "      <td>92.3445</td>\n",
       "      <td>150.7200</td>\n",
       "      <td>32.8212</td>\n",
       "      <td>66.7435</td>\n",
       "      <td>323.310</td>\n",
       "      <td>71.2947</td>\n",
       "      <td>126.4890</td>\n",
       "      <td>58.3018</td>\n",
       "      <td>54.5501</td>\n",
       "      <td>...</td>\n",
       "      <td>8.1399</td>\n",
       "      <td>25.1636</td>\n",
       "      <td>13.80520</td>\n",
       "      <td>15.8045</td>\n",
       "      <td>183.30</td>\n",
       "      <td>56.260</td>\n",
       "      <td>112.4090</td>\n",
       "      <td>10.800</td>\n",
       "      <td>87.725</td>\n",
       "      <td>84.9815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>71.0911</td>\n",
       "      <td>92.3920</td>\n",
       "      <td>150.8290</td>\n",
       "      <td>32.8869</td>\n",
       "      <td>66.7276</td>\n",
       "      <td>323.570</td>\n",
       "      <td>71.6364</td>\n",
       "      <td>127.2170</td>\n",
       "      <td>58.5536</td>\n",
       "      <td>54.7601</td>\n",
       "      <td>...</td>\n",
       "      <td>8.2083</td>\n",
       "      <td>25.3208</td>\n",
       "      <td>13.82690</td>\n",
       "      <td>15.6810</td>\n",
       "      <td>183.46</td>\n",
       "      <td>55.960</td>\n",
       "      <td>112.3890</td>\n",
       "      <td>10.870</td>\n",
       "      <td>87.855</td>\n",
       "      <td>85.4150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows × 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker      AAPL.US  AMZN.US   MSFT.US   BAC.US  GOOGL.US  NFLX.US     C.US  \\\n",
       "date                                                                          \n",
       "2015-01-02  24.3203  15.4260   40.1536  15.3668   26.3819   49.849  45.0718   \n",
       "2015-01-05  23.6361  15.1095   39.7826  14.9238   25.8791   47.311  43.6485   \n",
       "2015-01-06  23.6381  14.7645   39.1972  14.4711   25.2406   46.501  42.1145   \n",
       "2015-01-07  23.9734  14.9210   39.6975  14.5438   25.1663   46.743  42.5031   \n",
       "2015-01-08  24.8927  15.0230   40.8676  14.8413   25.2540   47.779  43.1443   \n",
       "...             ...      ...       ...      ...       ...      ...      ...   \n",
       "2019-12-24  68.8225  89.4605  150.5140  32.8869   66.9786  333.200  70.4716   \n",
       "2019-12-26  70.1874  93.4385  151.7760  33.1679   67.8774  332.630  71.5816   \n",
       "2019-12-27  70.1597  93.4900  152.0480  33.0077   67.4874  329.090  71.4387   \n",
       "2019-12-30  70.5778  92.3445  150.7200  32.8212   66.7435  323.310  71.2947   \n",
       "2019-12-31  71.0911  92.3920  150.8290  32.8869   66.7276  323.570  71.6364   \n",
       "\n",
       "ticker        JPM.US   XOM.US  INTC.US  ...  CLF.US  FITB.US    GEN.US  \\\n",
       "date                                    ...                              \n",
       "2015-01-02   50.3059  63.7264  28.9303  ...  6.6967  14.5194  10.27600   \n",
       "2015-01-05   48.7429  61.9827  28.6040  ...  6.2108  14.0833  10.13220   \n",
       "2015-01-06   47.4804  61.6540  28.0687  ...  6.0109  13.5169   9.97376   \n",
       "2015-01-07   47.5512  62.2793  28.6587  ...  6.5157  13.6660  10.06520   \n",
       "2015-01-08   48.6122  63.3131  29.1950  ...  6.9825  13.9755  10.24930   \n",
       "...              ...      ...      ...  ...     ...      ...       ...   \n",
       "2019-12-24  125.5370  58.7539  54.3557  ...  8.2083  25.3039  13.91340   \n",
       "2019-12-26  126.8730  58.8474  54.7318  ...  8.1692  25.4341  13.78850   \n",
       "2019-12-27  126.9610  58.6451  54.9702  ...  8.0324  25.2543  13.87010   \n",
       "2019-12-30  126.4890  58.3018  54.5501  ...  8.1399  25.1636  13.80520   \n",
       "2019-12-31  127.2170  58.5536  54.7601  ...  8.2083  25.3208  13.82690   \n",
       "\n",
       "ticker       GAP.US  ADSK.US  FSLR.US    ADI.US  PCG.US  NVS.US   AEP.US  \n",
       "date                                                                      \n",
       "2015-01-02  31.2373    59.53   44.545   46.7245  48.772  73.049  46.3700  \n",
       "2015-01-05  31.4391    58.66   41.830   45.8705  49.594  73.210  45.6898  \n",
       "2015-01-06  31.0230    57.50   40.860   44.7955  49.519  72.591  45.9386  \n",
       "2015-01-07  32.5231    57.38   41.750   45.2680  49.913  72.907  46.5922  \n",
       "2015-01-08  32.1495    58.80   43.630   46.0677  50.543  75.442  46.9913  \n",
       "...             ...      ...      ...       ...     ...     ...      ...  \n",
       "2019-12-24  15.6452   183.91   57.980  113.2370  10.950  87.976  84.8095  \n",
       "2019-12-26  15.9020   184.24   58.660  113.2470  10.860  87.966  84.8459  \n",
       "2019-12-27  15.8131   185.38   56.410  112.9800  10.440  88.486  85.1279  \n",
       "2019-12-30  15.8045   183.30   56.260  112.4090  10.800  87.725  84.9815  \n",
       "2019-12-31  15.6810   183.46   55.960  112.3890  10.870  87.855  85.4150  \n",
       "\n",
       "[1258 rows x 172 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see notebook 05_cointagration_tests\n",
    "stocks = pd.read_hdf('data.h5', 'stocks/close').loc['2015':]\n",
    "stocks.info()\n",
    "stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ETF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:42:48.064535Z",
     "start_time": "2020-06-19T02:42:48.044660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1258 entries, 2015-01-02 to 2019-12-31\n",
      "Columns: 132 entries, SPY.US to VNM.US\n",
      "dtypes: float64(132)\n",
      "memory usage: 1.3 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>SPY.US</th>\n",
       "      <th>EEM.US</th>\n",
       "      <th>GLD.US</th>\n",
       "      <th>EFA.US</th>\n",
       "      <th>XLF.US</th>\n",
       "      <th>XLE.US</th>\n",
       "      <th>TLT.US</th>\n",
       "      <th>GDX.US</th>\n",
       "      <th>EWZ.US</th>\n",
       "      <th>HYG.US</th>\n",
       "      <th>...</th>\n",
       "      <th>EPU.US</th>\n",
       "      <th>WIP.US</th>\n",
       "      <th>PJP.US</th>\n",
       "      <th>INDY.US</th>\n",
       "      <th>XPH.US</th>\n",
       "      <th>STPZ.US</th>\n",
       "      <th>BRF.US</th>\n",
       "      <th>IDX.US</th>\n",
       "      <th>EWN.US</th>\n",
       "      <th>VNM.US</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>172.069</td>\n",
       "      <td>31.2787</td>\n",
       "      <td>114.08</td>\n",
       "      <td>49.2607</td>\n",
       "      <td>17.2861</td>\n",
       "      <td>58.0695</td>\n",
       "      <td>97.6041</td>\n",
       "      <td>17.6428</td>\n",
       "      <td>28.5620</td>\n",
       "      <td>65.7660</td>\n",
       "      <td>...</td>\n",
       "      <td>26.3338</td>\n",
       "      <td>48.3205</td>\n",
       "      <td>59.5551</td>\n",
       "      <td>27.644</td>\n",
       "      <td>47.7751</td>\n",
       "      <td>46.5885</td>\n",
       "      <td>16.212</td>\n",
       "      <td>21.057</td>\n",
       "      <td>20.6083</td>\n",
       "      <td>17.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>168.957</td>\n",
       "      <td>30.7209</td>\n",
       "      <td>115.80</td>\n",
       "      <td>48.0975</td>\n",
       "      <td>16.9218</td>\n",
       "      <td>55.6700</td>\n",
       "      <td>99.0879</td>\n",
       "      <td>18.1069</td>\n",
       "      <td>27.5829</td>\n",
       "      <td>65.1567</td>\n",
       "      <td>...</td>\n",
       "      <td>25.9352</td>\n",
       "      <td>48.2820</td>\n",
       "      <td>59.1721</td>\n",
       "      <td>27.327</td>\n",
       "      <td>47.4247</td>\n",
       "      <td>46.5290</td>\n",
       "      <td>15.602</td>\n",
       "      <td>20.633</td>\n",
       "      <td>19.9071</td>\n",
       "      <td>17.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>167.378</td>\n",
       "      <td>30.5919</td>\n",
       "      <td>117.12</td>\n",
       "      <td>47.5535</td>\n",
       "      <td>16.6656</td>\n",
       "      <td>54.8522</td>\n",
       "      <td>100.9170</td>\n",
       "      <td>19.0656</td>\n",
       "      <td>28.0758</td>\n",
       "      <td>64.9076</td>\n",
       "      <td>...</td>\n",
       "      <td>26.1610</td>\n",
       "      <td>48.1745</td>\n",
       "      <td>58.8240</td>\n",
       "      <td>26.463</td>\n",
       "      <td>46.9928</td>\n",
       "      <td>46.4249</td>\n",
       "      <td>15.451</td>\n",
       "      <td>20.633</td>\n",
       "      <td>19.6988</td>\n",
       "      <td>17.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>169.435</td>\n",
       "      <td>31.2549</td>\n",
       "      <td>116.43</td>\n",
       "      <td>48.0818</td>\n",
       "      <td>16.8398</td>\n",
       "      <td>54.9686</td>\n",
       "      <td>100.6960</td>\n",
       "      <td>18.7126</td>\n",
       "      <td>28.8371</td>\n",
       "      <td>65.3095</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0954</td>\n",
       "      <td>47.9929</td>\n",
       "      <td>60.1818</td>\n",
       "      <td>26.888</td>\n",
       "      <td>48.1821</td>\n",
       "      <td>46.4834</td>\n",
       "      <td>15.720</td>\n",
       "      <td>20.977</td>\n",
       "      <td>19.8706</td>\n",
       "      <td>17.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>172.469</td>\n",
       "      <td>31.7878</td>\n",
       "      <td>115.94</td>\n",
       "      <td>48.7324</td>\n",
       "      <td>17.0901</td>\n",
       "      <td>56.2021</td>\n",
       "      <td>99.3622</td>\n",
       "      <td>18.4333</td>\n",
       "      <td>29.2975</td>\n",
       "      <td>65.8027</td>\n",
       "      <td>...</td>\n",
       "      <td>26.3936</td>\n",
       "      <td>48.0728</td>\n",
       "      <td>61.4590</td>\n",
       "      <td>27.718</td>\n",
       "      <td>49.2462</td>\n",
       "      <td>46.5637</td>\n",
       "      <td>15.943</td>\n",
       "      <td>21.074</td>\n",
       "      <td>20.1767</td>\n",
       "      <td>17.213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>296.904</td>\n",
       "      <td>39.9678</td>\n",
       "      <td>141.27</td>\n",
       "      <td>65.1116</td>\n",
       "      <td>28.9749</td>\n",
       "      <td>52.2111</td>\n",
       "      <td>118.6900</td>\n",
       "      <td>27.5556</td>\n",
       "      <td>42.7073</td>\n",
       "      <td>83.1442</td>\n",
       "      <td>...</td>\n",
       "      <td>33.4480</td>\n",
       "      <td>52.1692</td>\n",
       "      <td>64.0911</td>\n",
       "      <td>36.100</td>\n",
       "      <td>45.3280</td>\n",
       "      <td>50.2998</td>\n",
       "      <td>25.342</td>\n",
       "      <td>22.084</td>\n",
       "      <td>32.4870</td>\n",
       "      <td>15.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>298.482</td>\n",
       "      <td>40.2535</td>\n",
       "      <td>142.38</td>\n",
       "      <td>65.3943</td>\n",
       "      <td>29.1333</td>\n",
       "      <td>52.1945</td>\n",
       "      <td>118.9800</td>\n",
       "      <td>27.9588</td>\n",
       "      <td>43.6511</td>\n",
       "      <td>83.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>33.8494</td>\n",
       "      <td>52.3300</td>\n",
       "      <td>63.6822</td>\n",
       "      <td>35.914</td>\n",
       "      <td>45.1433</td>\n",
       "      <td>50.3583</td>\n",
       "      <td>25.867</td>\n",
       "      <td>22.172</td>\n",
       "      <td>32.6914</td>\n",
       "      <td>15.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>298.405</td>\n",
       "      <td>40.4156</td>\n",
       "      <td>142.33</td>\n",
       "      <td>65.5068</td>\n",
       "      <td>29.0591</td>\n",
       "      <td>51.9753</td>\n",
       "      <td>119.1090</td>\n",
       "      <td>27.7572</td>\n",
       "      <td>43.3034</td>\n",
       "      <td>83.2316</td>\n",
       "      <td>...</td>\n",
       "      <td>33.7954</td>\n",
       "      <td>52.5876</td>\n",
       "      <td>63.3112</td>\n",
       "      <td>35.942</td>\n",
       "      <td>44.6361</td>\n",
       "      <td>50.3583</td>\n",
       "      <td>26.039</td>\n",
       "      <td>22.129</td>\n",
       "      <td>32.7585</td>\n",
       "      <td>15.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>296.759</td>\n",
       "      <td>40.1466</td>\n",
       "      <td>142.63</td>\n",
       "      <td>65.0167</td>\n",
       "      <td>28.9749</td>\n",
       "      <td>51.8110</td>\n",
       "      <td>118.6800</td>\n",
       "      <td>28.3540</td>\n",
       "      <td>43.2939</td>\n",
       "      <td>83.1730</td>\n",
       "      <td>...</td>\n",
       "      <td>33.8407</td>\n",
       "      <td>52.5116</td>\n",
       "      <td>62.8228</td>\n",
       "      <td>35.989</td>\n",
       "      <td>44.0930</td>\n",
       "      <td>50.4158</td>\n",
       "      <td>26.115</td>\n",
       "      <td>21.909</td>\n",
       "      <td>32.6519</td>\n",
       "      <td>15.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>297.478</td>\n",
       "      <td>40.2361</td>\n",
       "      <td>142.90</td>\n",
       "      <td>65.3180</td>\n",
       "      <td>29.0681</td>\n",
       "      <td>52.1045</td>\n",
       "      <td>117.5120</td>\n",
       "      <td>28.1515</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>83.2494</td>\n",
       "      <td>...</td>\n",
       "      <td>33.8687</td>\n",
       "      <td>52.5807</td>\n",
       "      <td>63.1351</td>\n",
       "      <td>36.008</td>\n",
       "      <td>44.4911</td>\n",
       "      <td>50.3871</td>\n",
       "      <td>26.182</td>\n",
       "      <td>22.016</td>\n",
       "      <td>32.7872</td>\n",
       "      <td>15.847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker       SPY.US   EEM.US  GLD.US   EFA.US   XLF.US   XLE.US    TLT.US  \\\n",
       "date                                                                        \n",
       "2015-01-02  172.069  31.2787  114.08  49.2607  17.2861  58.0695   97.6041   \n",
       "2015-01-05  168.957  30.7209  115.80  48.0975  16.9218  55.6700   99.0879   \n",
       "2015-01-06  167.378  30.5919  117.12  47.5535  16.6656  54.8522  100.9170   \n",
       "2015-01-07  169.435  31.2549  116.43  48.0818  16.8398  54.9686  100.6960   \n",
       "2015-01-08  172.469  31.7878  115.94  48.7324  17.0901  56.2021   99.3622   \n",
       "...             ...      ...     ...      ...      ...      ...       ...   \n",
       "2019-12-24  296.904  39.9678  141.27  65.1116  28.9749  52.2111  118.6900   \n",
       "2019-12-26  298.482  40.2535  142.38  65.3943  29.1333  52.1945  118.9800   \n",
       "2019-12-27  298.405  40.4156  142.33  65.5068  29.0591  51.9753  119.1090   \n",
       "2019-12-30  296.759  40.1466  142.63  65.0167  28.9749  51.8110  118.6800   \n",
       "2019-12-31  297.478  40.2361  142.90  65.3180  29.0681  52.1045  117.5120   \n",
       "\n",
       "ticker       GDX.US   EWZ.US   HYG.US  ...   EPU.US   WIP.US   PJP.US  \\\n",
       "date                                   ...                              \n",
       "2015-01-02  17.6428  28.5620  65.7660  ...  26.3338  48.3205  59.5551   \n",
       "2015-01-05  18.1069  27.5829  65.1567  ...  25.9352  48.2820  59.1721   \n",
       "2015-01-06  19.0656  28.0758  64.9076  ...  26.1610  48.1745  58.8240   \n",
       "2015-01-07  18.7126  28.8371  65.3095  ...  26.0954  47.9929  60.1818   \n",
       "2015-01-08  18.4333  29.2975  65.8027  ...  26.3936  48.0728  61.4590   \n",
       "...             ...      ...      ...  ...      ...      ...      ...   \n",
       "2019-12-24  27.5556  42.7073  83.1442  ...  33.4480  52.1692  64.0911   \n",
       "2019-12-26  27.9588  43.6511  83.2684  ...  33.8494  52.3300  63.6822   \n",
       "2019-12-27  27.7572  43.3034  83.2316  ...  33.7954  52.5876  63.3112   \n",
       "2019-12-30  28.3540  43.2939  83.1730  ...  33.8407  52.5116  62.8228   \n",
       "2019-12-31  28.1515  43.5231  83.2494  ...  33.8687  52.5807  63.1351   \n",
       "\n",
       "ticker      INDY.US   XPH.US  STPZ.US  BRF.US  IDX.US   EWN.US  VNM.US  \n",
       "date                                                                    \n",
       "2015-01-02   27.644  47.7751  46.5885  16.212  21.057  20.6083  17.332  \n",
       "2015-01-05   27.327  47.4247  46.5290  15.602  20.633  19.9071  17.004  \n",
       "2015-01-06   26.463  46.9928  46.4249  15.451  20.633  19.6988  17.242  \n",
       "2015-01-07   26.888  48.1821  46.4834  15.720  20.977  19.8706  17.195  \n",
       "2015-01-08   27.718  49.2462  46.5637  15.943  21.074  20.1767  17.213  \n",
       "...             ...      ...      ...     ...     ...      ...     ...  \n",
       "2019-12-24   36.100  45.3280  50.2998  25.342  22.084  32.4870  15.688  \n",
       "2019-12-26   35.914  45.1433  50.3583  25.867  22.172  32.6914  15.708  \n",
       "2019-12-27   35.942  44.6361  50.3583  26.039  22.129  32.7585  15.708  \n",
       "2019-12-30   35.989  44.0930  50.4158  26.115  21.909  32.6519  15.767  \n",
       "2019-12-31   36.008  44.4911  50.3871  26.182  22.016  32.7872  15.847  \n",
       "\n",
       "[1258 rows x 132 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see notebook 05_cointagration_tests\n",
    "etfs = pd.read_hdf('data.h5', 'etfs/close').loc['2015':]\n",
    "etfs.info()\n",
    "etfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Ticker Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:42:48.072961Z",
     "start_time": "2020-06-19T02:42:48.066585Z"
    }
   },
   "outputs": [],
   "source": [
    "names = pd.read_hdf('data.h5', 'tickers').to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:42:48.083843Z",
     "start_time": "2020-06-19T02:42:48.074540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(304)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(names).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precompute Cointegration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:42:48.095421Z",
     "start_time": "2020-06-19T02:42:48.085219Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_cointegration(etfs, stocks, test_end, lookback=2):\n",
    "    start = time()\n",
    "    results = []\n",
    "    test_start = test_end - pd.DateOffset(years=lookback) + pd.DateOffset(days=1)\n",
    "    \n",
    "    print(f\"Test Start Date: {test_start.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Test End Date  : {test_end.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    etf_tickers = etfs.columns.tolist()\n",
    "    etf_data = etfs.loc[str(test_start):str(test_end)]\n",
    "\n",
    "    stock_tickers = stocks.columns.tolist()\n",
    "    stock_data = stocks.loc[str(test_start):str(test_end)]\n",
    "    \n",
    "    print(f\"Number of samples (days) in range: {len(etf_data)}\")\n",
    "    \n",
    "    n = len(etf_tickers) * len(stock_tickers)\n",
    "    j = 0\n",
    "    for i, s1 in enumerate(etf_tickers, 1):\n",
    "        for s2 in stock_tickers:\n",
    "            j += 1\n",
    "            if j % 1000 == 0:\n",
    "                print(f'\\t{j:5,.0f} ({j/n:3.1%}) | {time() - start:.2f}')\n",
    "            df = etf_data.loc[:, [s1]].dropna().join(stock_data.loc[:, [s2]].dropna(), how='inner')\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                var = VAR(df)\n",
    "                lags = var.select_order()\n",
    "                result = [test_end, s1, s2]\n",
    "                order = lags.selected_orders['aic']\n",
    "                result += [coint(df[s1], df[s2], trend='c')[1], coint(df[s2], df[s1], trend='c')[1]]\n",
    "\n",
    "            cj = coint_johansen(df, det_order=0, k_ar_diff=order)\n",
    "            result += (list(cj.lr1) + list(cj.lr2) + list(cj.evec[:, cj.ind[0]]))\n",
    "            results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Test Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:42:48.110785Z",
     "start_time": "2020-06-19T02:42:48.096915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2016-12-31', '2017-03-31', '2017-06-30', '2017-09-30',\n",
       "               '2017-12-31', '2018-03-31', '2018-06-30', '2018-09-30',\n",
       "               '2018-12-31', '2019-03-31', '2019-06-30'],\n",
       "              dtype='datetime64[ns]', name='date', freq='QE-DEC')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = stocks.loc['2016-12':'2019-6'].resample('Q').last().index\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:29.759471Z",
     "start_time": "2020-06-19T02:42:48.111942Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-31 00:00:00\n",
      "Test Start Date: 2015-01-01\n",
      "Test End Date  : 2016-12-31\n",
      "Number of samples (days) in range: 504\n",
      "\t1,000 (4.4%) | 21.12\n",
      "\t2,000 (8.8%) | 42.27\n",
      "\t3,000 (13.2%) | 63.61\n",
      "\t4,000 (17.6%) | 84.77\n",
      "\t5,000 (22.0%) | 106.33\n",
      "\t6,000 (26.4%) | 128.35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_end \u001b[38;5;129;01min\u001b[39;00m dates:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(test_end)\n\u001b[0;32m----> 8\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtest_cointegration\u001b[49m\u001b[43m(\u001b[49m\u001b[43metfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     test_results\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mDataFrame(result, columns\u001b[38;5;241m=\u001b[39mcolumns))\n\u001b[1;32m     11\u001b[0m pd\u001b[38;5;241m.\u001b[39mconcat(test_results)\u001b[38;5;241m.\u001b[39mto_hdf(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbacktest.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcointegration_test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m, in \u001b[0;36mtest_cointegration\u001b[0;34m(etfs, stocks, test_end, lookback)\u001b[0m\n\u001b[1;32m     26\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m var \u001b[38;5;241m=\u001b[39m VAR(df)\n\u001b[0;32m---> 28\u001b[0m lags \u001b[38;5;241m=\u001b[39m \u001b[43mvar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m result \u001b[38;5;241m=\u001b[39m [test_end, s1, s2]\n\u001b[1;32m     30\u001b[0m order \u001b[38;5;241m=\u001b[39m lags\u001b[38;5;241m.\u001b[39mselected_orders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maic\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/crypto/lib/python3.12/site-packages/statsmodels/tsa/vector_ar/var_model.py:821\u001b[0m, in \u001b[0;36mVAR.select_order\u001b[0;34m(self, maxlags, trend)\u001b[0m\n\u001b[1;32m    817\u001b[0m p_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m trend \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(p_min, maxlags \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;66;03m# exclude some periods to same amount of data used for each lag\u001b[39;00m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# order\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxlags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39minfo_criteria\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    824\u001b[0m         ics[k]\u001b[38;5;241m.\u001b[39mappend(v)\n",
      "File \u001b[0;32m~/miniconda3/envs/crypto/lib/python3.12/site-packages/statsmodels/tsa/vector_ar/var_model.py:742\u001b[0m, in \u001b[0;36mVAR._estimate_var\u001b[0;34m(self, lags, offset, trend)\u001b[0m\n\u001b[1;32m    740\u001b[0m y_sample \u001b[38;5;241m=\u001b[39m endog[lags:]\n\u001b[1;32m    741\u001b[0m \u001b[38;5;66;03m# Lütkepohl p75, about 5x faster than stated formula\u001b[39;00m\n\u001b[0;32m--> 742\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-15\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    743\u001b[0m resid \u001b[38;5;241m=\u001b[39m y_sample \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(z, params)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# Unbiased estimate of covariance matrix $\\Sigma_u$ of the white noise\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# process $u$\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# equivalent definition\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;66;03m# Ref: Lütkepohl p.75\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;66;03m# df_resid right now is T - Kp - 1, which is a suggested correction\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/crypto/lib/python3.12/site-packages/numpy/linalg/_linalg.py:2565\u001b[0m, in \u001b[0;36mlstsq\u001b[0;34m(a, b, rcond)\u001b[0m\n\u001b[1;32m   2561\u001b[0m     b \u001b[38;5;241m=\u001b[39m zeros(b\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (m, n_rhs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mb\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call\u001b[38;5;241m=\u001b[39m_raise_linalgerror_lstsq, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2564\u001b[0m               over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m-> 2565\u001b[0m     x, resids, rank, s \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2568\u001b[0m     x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # NON PARALLEL VERSION\n",
    "# test_results = []\n",
    "# columns = ['test_end', 's1', 's2', 'eg1', 'eg2',\n",
    "#            'trace0', 'trace1', 'eig0', 'eig1', 'w1', 'w2']\n",
    "\n",
    "# for test_end in dates:\n",
    "#     print(test_end)\n",
    "#     result = test_cointegration(etfs, stocks, test_end=test_end)\n",
    "#     test_results.append(pd.DataFrame(result, columns=columns))\n",
    "\n",
    "# pd.concat(test_results).to_hdf('backtest.h5', 'cointegration_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to parallelize the loop over `dates` using `joblib.Parallel` and `delayed`, as the computations for each `test_end` appear to be independent. This can provide a speedup if you have multiple CPU cores available, though the benefit may be limited if the number of `dates` is small (e.g., ~10-11 in your example) relative to the overhead of parallelization. The heavy lifting is inside `test_cointegration` (the nested loops over tickers), but since your question focuses on the outer loop, here's how to modify it.\n",
    "\n",
    "### Key Notes:\n",
    "- `n_jobs=-1` uses all available CPU cores. You can set it to a specific number (e.g., `n_jobs=4`) if you want to limit it.\n",
    "- `verbose=10` will print progress information from joblib (e.g., which jobs are running). Adjust or remove if not needed. The `print(test_end)` in your original loop won't execute sequentially in parallel, but the verbose output can serve a similar purpose.\n",
    "- The inner `print` statements in `test_cointegration` (e.g., every 1000 iterations) may appear interleaved or out of order in the console due to parallel execution, but this won't affect the results.\n",
    "- Ensure your environment supports multiprocessing (e.g., no issues with shared resources like the `etfs` and `stocks` DataFrames, which are read-only here).\n",
    "- If the speedup isn't sufficient, consider parallelizing the inner nested loops in `test_cointegration` instead (e.g., over pairs of tickers), as that's likely where most time is spent. That could be done similarly with `Parallel` but would require refactoring to generate the list of (s1, s2) pairs upfront.\n",
    "\n",
    "\n",
    "11m45s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PARALLEL VERSION\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# test_results = []\n",
    "# columns = ['test_end', 's1', 's2', 'eg1', 'eg2',\n",
    "#            'trace0', 'trace1', 'eig0', 'eig1', 'w1', 'w2']\n",
    "\n",
    "# # Parallelize the loop over dates\n",
    "# results = Parallel(n_jobs=-1, verbose=10)(\n",
    "#     delayed(test_cointegration)(etfs, stocks, test_end=test_end) for test_end in dates\n",
    "# )\n",
    "\n",
    "# # Convert each result to a DataFrame\n",
    "# test_dfs = [pd.DataFrame(result, columns=columns) for result in results]\n",
    "\n",
    "# # Concatenate and save\n",
    "# pd.concat(test_dfs).to_hdf('backtest.h5', 'cointegration_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload  Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column Definitions for Cointegration Tests\n",
    "==========================================\n",
    "\n",
    "The list `columns = ['test_end', 's1', 's2', 'eg1', 'eg2', 'trace0', 'trace1', 'eig0', 'eig1', 'w1', 'w2']` defines columns for a DataFrame storing results from cointegration tests on ETF-stock pairs, run in `test_cointegration` over a 2-year lookback period ending at `test_end`. Cointegration indicates a stable long-term relationship between non-stationary time series, useful for pairs trading. Tests include Engle-Granger (EG) and Johansen methods.\n",
    "\n",
    "-   **test_end**: End date of the test period (quarterly, 2016-12 to 2019-06). Input to `test_cointegration`.\n",
    "-   **s1**: ETF ticker (e.g., 'SPY'). From `etfs` DataFrame columns.\n",
    "-   **s2**: Stock ticker (e.g., 'AAPL'). From `stocks` DataFrame columns.\n",
    "-   **eg1**: P-value from EG test with s1 as dependent (regressed on s2). From `coint(df[s1], df[s2], trend='c')[1]`. Low p-value (<0.05) suggests cointegration.\n",
    "-   **eg2**: P-value from EG test with s2 as dependent (regressed on s1). From `coint(df[s2], df[s1], trend='c')[1]`. Checks reverse direction.\n",
    "-   **trace0**: Johansen trace statistic for H0: r=0 (no cointegration). From `coint_johansen(df, det_order=0, k_ar_diff=order).lr1[0]`. If > 15.4943 (95% critical value), reject H0, indicating cointegration.\n",
    "-   **trace1**: Johansen trace statistic for H0: r≤1. From `coint_johansen(...).lr1[1]`. For pairs, r>1 is impossible (saturation), so typically < 3.8415 (95% critical value), confirming at most one relation.\n",
    "-   **eig0**: Johansen max-eigenvalue statistic for r=0. From `coint_johansen(...).lr2[0]`. High values suggest cointegration.\n",
    "-   **eig1**: Johansen max-eigenvalue statistic for r=1. From `coint_johansen(...).lr2[1]`. Tests impossible r=2, usually insignificant.\n",
    "-   **w1**: Cointegrating vector weight for s1. From `coint_johansen(...).evec[:, cj.ind[0]][0]`. Part of β vector for stationary combination.\n",
    "-   **w2**: Cointegrating vector weight for s2. From `coint_johansen(...).evec[:, cj.ind[0]][1]`. With `w1`, forms hedge ratio for trading.\n",
    "\n",
    "### Notes on `trace0` and `trace1`\n",
    "\n",
    "For pairs (two series), the Johansen test has a maximum rank of 1. `trace0` tests for cointegration (r>0); `trace1` tests for r>1 (impossible, hence \"saturation\"). A high `trace0` with low `trace1` suggests exactly one cointegrating relation. Results are saved to 'backtest.h5' for analysis, parallelized over quarterly periods using Joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:29.819729Z",
     "start_time": "2020-06-19T06:01:29.760347Z"
    }
   },
   "outputs": [],
   "source": [
    "test_results = pd.read_hdf('backtest.h5', 'cointegration_test')\n",
    "test_results.info()\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Cointegrated Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significant Johansen Trace Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results['joh_sig'] = ((test_results.trace0 > trace0_cv) &\n",
    "                           (test_results.trace1 < trace1_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:29.824433Z",
     "start_time": "2020-06-19T06:01:29.820808Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_results['joh_sig'] = ((test_results.trace0 > trace0_cv) &\n",
    "#                            (test_results.trace1 > trace1_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:29.834934Z",
     "start_time": "2020-06-19T06:01:29.825442Z"
    }
   },
   "outputs": [],
   "source": [
    "test_results.joh_sig.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significant Engle Granger Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:29.846685Z",
     "start_time": "2020-06-19T06:01:29.835871Z"
    }
   },
   "outputs": [],
   "source": [
    "test_results['eg'] = test_results[['eg1', 'eg2']].min(axis=1)\n",
    "test_results['s1_dep'] = test_results.eg1 < test_results.eg2\n",
    "test_results['eg_sig'] = (test_results.eg < .05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:29.853515Z",
     "start_time": "2020-06-19T06:01:29.847666Z"
    }
   },
   "outputs": [],
   "source": [
    "test_results.eg_sig.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Engle-Granger vs Johansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:29.860981Z",
     "start_time": "2020-06-19T06:01:29.854361Z"
    }
   },
   "outputs": [],
   "source": [
    "test_results['coint'] = (test_results.eg_sig & test_results.joh_sig)\n",
    "test_results.coint.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:29.899199Z",
     "start_time": "2020-06-19T06:01:29.861982Z"
    }
   },
   "outputs": [],
   "source": [
    "test_results = test_results.drop(['eg1', 'eg2', 'trace0', 'trace1', 'eig0', 'eig1'], axis=1)\n",
    "test_results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "### Data Being Plotted\n",
    "- `test_results.groupby('test_end').coint.mean()`: Groups by quarterly `test_end` dates and computes the mean of the boolean `coint` column, yielding the proportion of ETF-stock pairs cointegrated per both Engle-Granger and Johansen tests.\n",
    "  - Example: For 100 pairs with 5 cointegrated, the proportion is 0.05 (5%).\n",
    "  - Computed over thousands of pairs (ETF-stock Cartesian product).\n",
    "- `.to_frame('# Pairs')`: Converts to a DataFrame; label is misleading as it shows proportions (e.g., 0.0–0.1), not counts (use `.sum()` for counts).\n",
    "\n",
    "### Plot Type\n",
    "- `.plot()`: Generates a line plot of proportions over time.\n",
    "  - X-axis: Quarterly `test_end` dates (~2016-12 to 2019-06).\n",
    "  - Y-axis: Proportion of cointegrated pairs (labeled '# Pairs').\n",
    "\n",
    "### Additional Element\n",
    "- `ax.axhline(.05, lw=1, ls='--', c='k')`: Horizontal dashed black line at y=0.05.\n",
    "  - Benchmark for 5% significance level: Expected false positives under null hypothesis of no cointegration.\n",
    "  - Line above 0.05 suggests genuine relationships (e.g., sector/market factors); at/below indicates noise or weak evidence.\n",
    "\n",
    "### Location in Code\n",
    "- Under \"### Comparison\", post-`coint` computation, pre-candidate selection.\n",
    "- Diagnostic to track cointegrated pair fraction over periods; complements overall proportion from `test_results.coint.value_counts(normalize=True)` (~0.01–0.05).\n",
    "\n",
    "### Graph Interpretation\n",
    "- **Expected Plot**: Line fluctuates ~0.01–0.10, often near/above 0.05; spikes/trends reflect market stability (more co-movement) vs. volatility.\n",
    "- **Strategic Relevance**: Assesses viable pairs for trading; low proportions (~0.05) risk false positives and poor backtests; precedes signal generation (e.g., z-score >2 entries).\n",
    "- **Issues/Insights**:\n",
    "  - **Multiple Testing**: High pair volume inflates false positives; >0.05 is positive but unadjusted (no Bonferroni here).\n",
    "  - **Time Variation**: Reveals stability (flat) or decay (downtrend), common in markets.\n",
    "  - **Comparisons**: Differs from count plots (e.g., `candidates.groupby('test_end').size()`) by benchmarking vs. chance.\n",
    "\n",
    "This graph diagnostically evaluates cointegration stability and significance in the pairs trading pipeline, prior to simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:30.054274Z",
     "start_time": "2020-06-19T06:01:29.900025Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = test_results.groupby('test_end').coint.mean().to_frame('Proportion').plot()\n",
    "ax.axhline(.05, lw=1, ls='--', c='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Candidate Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:30.058145Z",
     "start_time": "2020-06-19T06:01:30.055126Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_candidate_pairs(data):\n",
    "    candidates = data[data.joh_sig | data.eg_sig]\n",
    "    candidates['y'] = candidates.apply(lambda x: x.s1 if x.s1_dep else x.s2, axis=1)\n",
    "    candidates['x'] = candidates.apply(lambda x: x.s2 if x.s1_dep else x.s1, axis=1)\n",
    "    return candidates.drop(['s1_dep', 's1', 's2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:31.623786Z",
     "start_time": "2020-06-19T06:01:30.059031Z"
    }
   },
   "outputs": [],
   "source": [
    "candidates = select_candidate_pairs(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:31.640534Z",
     "start_time": "2020-06-19T06:01:31.624849Z"
    }
   },
   "outputs": [],
   "source": [
    "candidates.to_hdf('backtest.h5', 'candidates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:31.664395Z",
     "start_time": "2020-06-19T06:01:31.641471Z"
    }
   },
   "outputs": [],
   "source": [
    "candidates = pd.read_hdf('backtest.h5', 'candidates')\n",
    "candidates.info()\n",
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candidates over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:31.795571Z",
     "start_time": "2020-06-19T06:01:31.665218Z"
    }
   },
   "outputs": [],
   "source": [
    "candidates.groupby('test_end').size().plot(figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Common Pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:31.804899Z",
     "start_time": "2020-06-19T06:01:31.796947Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore('data.h5') as store:\n",
    "    print(store.info())\n",
    "    tickers = store['tickers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:31.824963Z",
     "start_time": "2020-06-19T06:01:31.805700Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore('backtest.h5') as store:\n",
    "    print(store.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `counter = Counter()` creates a `Counter` object that tallies the frequency of each unique sorted pair `(ticker1, ticker2)` (with `ticker1 < ticker2` lexicographically) among the candidate pairs where both `joh_sig` (Johansen significance) and `eg_sig` (Engle-Granger significance) are True.\n",
    "\n",
    "Each count represents the number of test periods (out of the 11 quarterly periods in the provided `DatetimeIndex`) in which that pair demonstrated cointegration according to both tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:31.861630Z",
     "start_time": "2020-06-19T06:01:31.825814Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for s1, s2 in zip(candidates[candidates.joh_sig & candidates.eg_sig].y, \n",
    "                  candidates[candidates.joh_sig & candidates.eg_sig].x):\n",
    "    if s1 > s2:\n",
    "        counter[(s2, s1)] += 1\n",
    "    else: \n",
    "        counter[(s1, s2)] += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:01:31.876602Z",
     "start_time": "2020-06-19T06:01:31.864124Z"
    }
   },
   "outputs": [],
   "source": [
    "most_common_pairs = pd.DataFrame(counter.most_common(10))\n",
    "most_common_pairs = pd.DataFrame(most_common_pairs[0].values.tolist(), columns=['s1', 's2'])\n",
    "most_common_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:10:24.702579Z",
     "start_time": "2020-06-19T13:10:24.618722Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore('backtest.h5') as store:\n",
    "    prices = store['prices'].close.unstack('ticker').ffill(limit=5)\n",
    "    tickers = store['tickers'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:10:24.901984Z",
     "start_time": "2020-06-19T13:10:24.785861Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt = pd.Series(counter).reset_index()\n",
    "cnt.columns = ['s1', 's2', 'n']\n",
    "cnt['name1'] = cnt.s1.map(tickers)\n",
    "cnt['name2'] = cnt.s2.map(tickers)\n",
    "cnt.nlargest(10, columns='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for i in range(len(most_common_pairs)):\n",
    "    # Get the tickers for the current pair\n",
    "    s1, s2 = most_common_pairs.at[i, 's1'], most_common_pairs.at[i, 's2']\n",
    "    \n",
    "    # Create a new figure for each pair\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Plot the price series for s1 and s2\n",
    "    prices.loc[:, [s1, s2]].rename(columns=tickers).plot(\n",
    "        secondary_y=tickers[s2],  # Second ticker on right y-axis\n",
    "        ax=ax,\n",
    "        rot=0  # Horizontal x-axis labels\n",
    "    )\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.grid(False)\n",
    "    ax.set_xlabel('')  # Remove x-axis label\n",
    "    ax.set_title(f'Price Series: {tickers[s1]} vs {tickers[s2]}')  # Add title with pair names\n",
    "    \n",
    "    # Clean up with Seaborn style\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:10:53.610381Z",
     "start_time": "2020-06-19T13:10:52.681128Z"
    }
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=5, nrows=2, figsize=(14, 5))\n",
    "# for i in [0, 1]:\n",
    "#     s1, s2 = most_common_pairs.at[i, 's1'], most_common_pairs.at[i, 's2']\n",
    "#     prices.loc[:, [s1, s2]].rename(columns=tickers).plot(secondary_y=tickers[s2],\n",
    "#                                                          ax=axes[i],\n",
    "#                                                          rot=0)\n",
    "#     axes[i].grid(False)\n",
    "#     axes[i].set_xlabel('')\n",
    "\n",
    "# sns.despine()\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Entry and Exit Dates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Key Functions in the Pairs Trading Code\n",
    "\n",
    "This code is part of a strategy for \"pairs trading,\" where you find two related assets (like an ETF and a stock) whose prices usually move together. When they temporarily drift apart (diverge), you bet they'll come back together (converge) by buying one and selling the other. The functions below help prepare the data for this: smoothing noisy prices, figuring out how to balance trades, estimating how quickly divergences fix themselves, and processing all pairs efficiently. I'll explain each in plain English, with examples tied to the code's context (e.g., using ETF 'SPY.US' and stock 'XOM.US' as a sample pair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFSmoother: Smoothing Prices to Reduce Noise\n",
    "This function uses a mathematical tool called a Kalman filter to \"smooth\" out the daily ups and downs in asset prices, making it easier to see the true underlying trend. It's like applying a smart moving average that adapts over time, ignoring short-term wiggles caused by market noise.\n",
    "\n",
    "- **Plain English Breakdown**:\n",
    "  - Prices in financial markets are bumpy due to random events (e.g., news or trades). This function estimates a cleaner version of the price series by assuming the \"true\" price evolves gradually but gets observed with some error.\n",
    "  - It starts with an initial guess (mean=0) and updates its estimate step by step as new price data comes in, balancing between the observed price and its prediction.\n",
    "  - The result is a smoothed series that's less volatile, which helps in later steps like calculating relationships between two assets.\n",
    "\n",
    "- **How It Works in Code**:\n",
    "  - It sets up a simple Kalman filter model: The price doesn't change much from day to day (transition matrix is identity), but there's a little wiggle room (covariance=0.05 for changes, 1 for observation noise).\n",
    "  - It runs the filter on the price values and returns a new series with the smoothed estimates.\n",
    "\n",
    "- **Example in Context**:\n",
    "  - Imagine raw prices for 'SPY.US' (S&P 500 ETF) over a week: [200, 202, 198, 205, 199]. These jump around.\n",
    "  - After smoothing: [200, 201, 200, 202, 201]. It's steadier, reducing noise.\n",
    "  - In the code, `smoothed_prices = prices.apply(KFSmoother)` applies this to every ticker's column in the `prices` DataFrame (e.g., all ETFs and stocks from 2016-2019). This preprocessed data is then used for hedge ratios and spreads, making the strategy more reliable than using raw, noisy prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:11:14.858886Z",
     "start_time": "2020-06-19T13:11:14.854069Z"
    }
   },
   "outputs": [],
   "source": [
    "def KFSmoother(prices):\n",
    "    \"\"\"Estimate rolling mean\"\"\"\n",
    "    \n",
    "    kf = KalmanFilter(transition_matrices=np.eye(1),\n",
    "                      observation_matrices=np.eye(1),\n",
    "                      initial_state_mean=0,\n",
    "                      initial_state_covariance=1,\n",
    "                      observation_covariance=1,\n",
    "                      transition_covariance=.05)\n",
    "\n",
    "    state_means, _ = kf.filter(prices.values)\n",
    "    return pd.Series(state_means.flatten(),\n",
    "                     index=prices.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:12:00.810776Z",
     "start_time": "2020-06-19T13:11:14.860508Z"
    }
   },
   "outputs": [],
   "source": [
    "smoothed_prices = prices.apply(KFSmoother)\n",
    "smoothed_prices.to_hdf('tmp.h5', 'smoothed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:12:00.820497Z",
     "start_time": "2020-06-19T13:12:00.812004Z"
    }
   },
   "outputs": [],
   "source": [
    "smoothed_prices = pd.read_hdf('tmp.h5', 'smoothed')\n",
    "smoothed_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFHedgeRatio: Calculating a Dynamic Balance for Trading Pairs\n",
    "This function figures out the \"hedge ratio\" – basically, how much of one asset you need to trade against the other to keep the pair balanced and neutral to overall market moves. It's dynamic, meaning it changes over time as market conditions shift, using another Kalman filter for adaptability.\n",
    "\n",
    "- **Plain English Breakdown**:\n",
    "  - In pairs trading, you don't just buy/sell equal amounts; one asset might be twice as volatile as the other, so you need a ratio (e.g., sell 2 units of Asset X for every 1 unit of Asset Y you buy) to cancel out common movements.\n",
    "  - This function treats the relationship as a changing linear equation (Y ≈ β * X + intercept), estimating β (the ratio) and the intercept over time.\n",
    "  - It returns a negative version of the estimates because the code later uses it to build the spread as Y + (negative β) * X, which is equivalent to Y - β * X.\n",
    "\n",
    "- **How It Works in Code**:\n",
    "  - It sets up a Kalman filter for a 2D state (β and intercept): They evolve slowly (small transition covariance based on delta=0.001).\n",
    "  - The observation matrix uses X and a constant (for intercept), and it filters based on Y's values.\n",
    "  - Output: A time series of -[β, intercept] for each day.\n",
    "\n",
    "- **Example in Context**:\n",
    "  - For pair Y='SPY.US' (dependent, price ~200) and X='XOM.US' (independent, price ~80):\n",
    "    - On Day 1, it might estimate β ≈ 2.5 (meaning SPY moves 2.5 times more than XOM in response to common factors).\n",
    "    - Hedge ratio returned: -2.5 (negative for spread formula).\n",
    "    - In the code, inside `process_pair`: `KFHedgeRatio(y=smoothed SPY prices, x=smoothed XOM prices)[:, 0]` gives the daily -β, used to compute the spread. If β=2.5, you might short 2.5 shares of XOM per share of SPY to hedge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def KFHedgeRatio(x, y):\n",
    "#     \"\"\"Estimate Hedge Ratio\"\"\"\n",
    "#     delta = 1e-3\n",
    "#     trans_cov = delta / (1 - delta) * np.eye(2)\n",
    "#     obs_mat = np.expand_dims(np.vstack([[x], [np.ones(len(x))]]).T, axis=1)\n",
    "\n",
    "#     kf = KalmanFilter(n_dim_obs=1, n_dim_state=2,\n",
    "#                       initial_state_mean=[0, 0],\n",
    "#                       initial_state_covariance=np.ones((2, 2)),\n",
    "#                       transition_matrices=np.eye(2),\n",
    "#                       observation_matrices=obs_mat,\n",
    "#                       observation_covariance=2,\n",
    "#                       transition_covariance=trans_cov)\n",
    "\n",
    "#     state_means, _ = kf.filter(y.values)\n",
    "#     return -state_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:12:00.829378Z",
     "start_time": "2020-06-19T13:12:00.821806Z"
    }
   },
   "outputs": [],
   "source": [
    "def KFHedgeRatio(x, y):\n",
    "    \"\"\"Estimate Hedge Ratio and Intercept\"\"\"\n",
    "    delta = 1e-3\n",
    "    trans_cov = delta / (1 - delta) * np.eye(2)\n",
    "    obs_mat = np.expand_dims(np.vstack([[x], [np.ones(len(x))]]).T, axis=1)\n",
    "\n",
    "    kf = KalmanFilter(n_dim_obs=1, n_dim_state=2,\n",
    "                      initial_state_mean=[0, 0],\n",
    "                      initial_state_covariance=np.ones((2, 2)),\n",
    "                      transition_matrices=np.eye(2),\n",
    "                      observation_matrices=obs_mat,\n",
    "                      observation_covariance=2,\n",
    "                      transition_covariance=trans_cov)\n",
    "\n",
    "    state_means, _ = kf.filter(y.values)\n",
    "    hedge_ratios = -state_means[:, 0]\n",
    "    intercepts = -state_means[:, 1]\n",
    "    return hedge_ratios, intercepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate mean reversion half life\n",
    "\n",
    "\n",
    "#### estimate_half_life: Estimating How Long Divergences Take to Fix Themselves\n",
    "This function calculates the \"half-life\" of a price spread – the average number of days it takes for a divergence between two assets to shrink by half, assuming it mean-reverts (comes back to normal). It's a measure of how quickly the pair corrects itself, which helps decide trading windows.\n",
    "\n",
    "- **Plain English Breakdown**:\n",
    "  - Mean-reverting spreads don't snap back instantly; they take time. Half-life tells you the speed: Short (e.g., 10 days) means fast fixes (good for quick trades); long (e.g., 200 days) means slow (riskier, as things might change).\n",
    "  - It fits a simple regression model to the spread's changes, estimating the reversion speed (beta), then converts it to days using a formula ($-ln(2)/beta$).\n",
    "  - Ensures a minimum of 1 day to avoid nonsense values.\n",
    "\n",
    "- **How It Works in Code**:\n",
    "  - Creates lagged spread (X) and differences (Y), adds a constant for intercept.\n",
    "  - Solves for beta using linear algebra (normal equation for OLS regression).\n",
    "  - Computes half-life and rounds/clamps it.\n",
    "\n",
    "- **Example in Context**:\n",
    "  - Suppose the spread (SPY - β * XOM) over 2 years: Starts at 0, jumps to 4, then slowly returns (e.g., 4 → 2 in 20 days, 2 → 1 in another 20).\n",
    "  - Beta might be -0.035 (negative indicates reversion), half-life ≈ 20 days (-0.693 / -0.035 ≈ 20).\n",
    "  - In the code, inside `process_pair`: `half_life = estimate_half_life(pair.spread.loc[t: test_end])` uses the formation period (2 years pre-trading). This 20-day value sets the rolling window for z-scores (min(40, max_window)), helping detect tradable divergences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:12:00.840770Z",
     "start_time": "2020-06-19T13:12:00.830166Z"
    }
   },
   "outputs": [],
   "source": [
    "def estimate_half_life(spread):\n",
    "    X = spread.shift().iloc[1:].to_frame().assign(const=1)\n",
    "    y = spread.diff().iloc[1:]\n",
    "    beta = (np.linalg.inv(X.T @ X) @ X.T @ y).iloc[0]\n",
    "    halflife = int(round(-np.log(2) / beta, 0))\n",
    "    return max(halflife, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Spread & Bollinger Bands\n",
    "\n",
    "#### get_spread_parallel: Processing All Pairs Efficiently in Parallel\n",
    "This is the main workhorse function that loops over time periods and candidate pairs, computing everything needed for trading (hedge ratios, spreads, half-lives, z-scores) using the above helpers. It runs in parallel to speed things up, as there are thousands of pairs.\n",
    "\n",
    "- **Plain English Breakdown**:\n",
    "  - It breaks the data into quarterly \"test periods\" (e.g., ending Dec 2016), grabs candidate pairs for each, and for a 2-year \"formation\" window plus 6-month \"trading\" window:\n",
    "    - Smooths prices, computes daily hedge ratios and spreads.\n",
    "    - Estimates half-life to size a rolling window.\n",
    "    - Calculates z-scores (how far the spread is from normal, in standard deviations) for spotting trades (e.g., z>2 means diverge, enter trade).\n",
    "  - Parallelizes per-pair work to handle scale (e.g., 1000+ pairs/period) quickly.\n",
    "\n",
    "- **How It Works in Code**:\n",
    "  - Outer loop: Over unique test_end dates (quarters from 2016-2019).\n",
    "  - For each period: Define time windows (t=2 years back, T=6 months forward).\n",
    "  - Inner function `process_pair`: For each pair (y,x), compute smoothed prices, hedge ratio, spread, half-life, rolling mean/std, z-score; output trading-period DataFrame and half-life list.\n",
    "  - Uses `joblib.Parallel` to run `process_pair` on all CPUs.\n",
    "  - Collects results into lists: `pairs` (DataFrames per pair-period) and `half_lives` (lists per pair).\n",
    "\n",
    "- **Example in Context**:\n",
    "  - For period ending 2016-12-31, with 1000 candidates (e.g., SPY-XOM as pair 1).\n",
    "    - Formation: 2015-01-01 to 2016-12-31; Trading: 2017-01-01 to 2017-06-30.\n",
    "    - For SPY-XOM: Smooth prices, get hedge ratios (e.g., -2.89 on Jan 3), spread (e.g., 2.50), half-life (19 days), z-score (e.g., -0.61 – not extreme).\n",
    "    - Outputs: A 125-row DataFrame (trading days) with these metrics for SPY-XOM, plus half-life [2016-12-31, 'SPY.US', 'XOM.US', 19].\n",
    "  - Full run: Produces `pairs` (list of ~thousands of such DataFrames) and `half_lives` (list of lists), used later for trade signals (e.g., enter if |z|>2).\n",
    "\n",
    "These functions work together: Smoothing cleans data, hedge ratio balances pairs, half-life tunes timing, and parallel processing makes it feasible for many pairs. In the strategy, this setup identifies profitable convergence opportunities while managing risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:12:00.854200Z",
     "start_time": "2020-06-19T13:12:00.841765Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_spread(candidates, prices):\n",
    "    pairs = []\n",
    "    half_lives = []\n",
    "\n",
    "    periods = pd.DatetimeIndex(sorted(candidates.test_end.unique()))\n",
    "    start = time()\n",
    "    for p, test_end in enumerate(periods, 1):\n",
    "        start_iteration = time()\n",
    "\n",
    "        period_candidates = candidates.loc[candidates.test_end == test_end, ['y', 'x']]\n",
    "        trading_start = test_end + pd.DateOffset(days=1)\n",
    "        t = trading_start - pd.DateOffset(years=2)\n",
    "        T = trading_start + pd.DateOffset(months=6) - pd.DateOffset(days=1)\n",
    "        max_window = len(prices.loc[t: test_end].index)\n",
    "        print(f\"max window: {max_window}\")\n",
    "        print(f\"test_end {test_end.date()}, {len(period_candidates)} pairs\")\n",
    "        for i, (y, x) in enumerate(zip(period_candidates.y, period_candidates.x), 1):\n",
    "            if i % 1000 == 0:\n",
    "                msg = f'{i:5.0f} | {time() - start_iteration:7.1f} | {time() - start:10.1f}'\n",
    "                print(msg)\n",
    "            pair = prices.loc[t: T, [y, x]]\n",
    "            pair['hedge_ratio'] = KFHedgeRatio(y=KFSmoother(prices.loc[t: T, y]),\n",
    "                                               x=KFSmoother(prices.loc[t: T, x]))[:, 0]\n",
    "            pair['spread'] = pair[y].add(pair[x].mul(pair.hedge_ratio))\n",
    "            half_life = estimate_half_life(pair.spread.loc[t: test_end])                \n",
    "            spread = pair.spread.rolling(window=min(2 * half_life, max_window))\n",
    "            print(f\"half_life {half_life} , spread window {min(2 * half_life, max_window)}\")\n",
    "            pair['z_score'] = pair.spread.sub(spread.mean()).div(spread.std())\n",
    "            pairs.append(pair.loc[trading_start: T].assign(s1=y, s2=x, period=p, pair=i).drop([x, y], axis=1))\n",
    "\n",
    "            half_lives.append([test_end, y, x, half_life])\n",
    "    return pairs, half_lives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T13:12:00.897593Z",
     "start_time": "2020-06-19T13:12:00.856141Z"
    }
   },
   "outputs": [],
   "source": [
    "candidates = pd.read_hdf('backtest.h5', 'candidates')\n",
    "candidates.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "48m 20.0s\n",
    "\n",
    "\n",
    "- **Max window:** 252, **Test end:** 2016-12-31, **Pairs:** 3497\n",
    "- **Max window:** 314, **Test end:** 2017-03-31, **Pairs:** 1978\n",
    "- **Max window:** 377, **Test end:** 2017-06-30, **Pairs:** 4124\n",
    "- **Max window:** 440, **Test end:** 2017-09-30, **Pairs:** 2024\n",
    "- **Max window:** 503, **Test end:** 2017-12-31, **Pairs:** 2885\n",
    "- **Max window:** 503, **Test end:** 2018-03-31, **Pairs:** 3513\n",
    "- **Max window:** 503, **Test end:** 2018-06-30, **Pairs:** 2399\n",
    "- **Max window:** 502, **Test end:** 2018-09-30, **Pairs:** 2929\n",
    "- **Max window:** 502, **Test end:** 2018-12-31, **Pairs:** 2846\n",
    "- **Max window:** 501, **Test end:** 2019-03-31, **Pairs:** 2606\n",
    "- **Max window:** 501, **Test end:** 2019-06-30, **Pairs:** 2645\n",
    "\n",
    "\n",
    "```python\n",
    "print(y, x) \n",
    "#SPY.US XOM.US\n",
    "```\n",
    "\n",
    "```python\n",
    "pair = prices.loc[t: T, [y, x]]\n",
    "print(pair)\n",
    "ticker         BMY.US     ILF.US\n",
    "date                            \n",
    "2016-07-01  60.032011  20.127575\n",
    "2016-07-05  60.274389  20.160980\n",
    "2016-07-06  60.481711  20.160264\n",
    "2016-07-07  60.683829  20.138191\n",
    "2016-07-08  61.045403  20.273513\n",
    "...               ...        ...\n",
    "2018-12-24  44.698739  25.344163\n",
    "2018-12-26  44.580871  25.357431\n",
    "2018-12-27  44.553777  25.403545\n",
    "2018-12-28  44.625562  25.489196\n",
    "2018-12-31  44.866089  25.577877\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:18:38.419560Z",
     "start_time": "2020-06-19T13:12:00.898765Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pairs, half_lives = get_spread(candidates, smoothed_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5m 4.0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `get_spread_parallel` function, the variables `trading_start`, `t`, and `T` define key dates for analyzing cointegrated pairs in a statistical arbitrage strategy:\n",
    "\n",
    "- **`trading_start`**: This is the date when trading begins for a given test period. It’s set to the day after the `test_end` date, marking the start of the trading window.\n",
    "- **`t`**: This is the start of the lookback period, exactly two years before `trading_start`. It defines the beginning of the historical data used to estimate the spread and hedge ratio.\n",
    "- **`T`**: This is the end of the trading window, six months after `trading_start`. It marks the cutoff date for the trading period being analyzed.\n",
    "\n",
    "In plain terms, `trading_start` is when you start trading, `t` is the start of the two-year historical data window used for calculations, and `T` is the end of the six-month trading period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# def get_spread_parallel(candidates, prices):\n",
    "#     pairs = []\n",
    "#     half_lives = []\n",
    "\n",
    "#     periods = pd.DatetimeIndex(sorted(candidates.test_end.unique()))\n",
    "#     start = time()\n",
    "#     for p, test_end in enumerate(periods, 1):\n",
    "#         start_iteration = time()\n",
    "\n",
    "#         period_candidates = candidates.loc[candidates.test_end == test_end, ['y', 'x']]\n",
    "#         trading_start = test_end + pd.DateOffset(days=1) #test end date + 1\n",
    "#         t = trading_start - pd.DateOffset(years=2) # 2 years before trading start\n",
    "#         T = trading_start + pd.DateOffset(months=6) - pd.DateOffset(days=1) # 6 months after trading start - 1 day\n",
    "#         max_window = len(prices.loc[t: test_end].index)\n",
    "#         print(test_end.date(), len(period_candidates))\n",
    "\n",
    "#         def process_pair(i, y, x):\n",
    "#             pair = prices.loc[t: T, [y, x]]\n",
    "#             pair['hedge_ratio'] = KFHedgeRatio(y=KFSmoother(prices.loc[t: T, y]),\n",
    "#                                                x=KFSmoother(prices.loc[t: T, x]))[:, 0]\n",
    "            \n",
    "#             pair['spread'] = pair[y].add(pair[x].mul(pair.hedge_ratio))\n",
    "#             half_life = estimate_half_life(pair.spread.loc[t: test_end])                \n",
    "\n",
    "#             spread = pair.spread.rolling(window=min(2 * half_life, max_window))\n",
    "#             pair['z_score'] = pair.spread.sub(spread.mean()).div(spread.std())\n",
    "#             pair_out = pair.loc[trading_start: T].assign(s1=y, s2=x, period=p, pair=i).drop([x, y], axis=1)\n",
    "\n",
    "#             hl_out = [test_end, y, x, half_life]\n",
    "#             return pair_out, hl_out\n",
    "\n",
    "#         pair_results = Parallel(n_jobs=-1, verbose=10)(\n",
    "#             delayed(process_pair)(i, y, x)\n",
    "#             for i, (y, x) in enumerate(zip(period_candidates.y, period_candidates.x), 1)\n",
    "#         )\n",
    "\n",
    "#         pairs.extend([pr[0] for pr in pair_results])\n",
    "#         half_lives.extend([pr[1] for pr in pair_results])\n",
    "\n",
    "#     return pairs, half_lives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def get_spread_parallel(candidates, prices):\n",
    "    pairs = []\n",
    "    half_lives = []\n",
    "\n",
    "    periods = pd.DatetimeIndex(sorted(candidates.test_end.unique()))\n",
    "    start = time()\n",
    "    for p, test_end in enumerate(periods, 1):\n",
    "        start_iteration = time()\n",
    "\n",
    "        period_candidates = candidates.loc[candidates.test_end == test_end, ['y', 'x']]\n",
    "        trading_start = test_end + pd.DateOffset(days=1) #test end date + 1\n",
    "        t = trading_start - pd.DateOffset(years=2) # 2 years before trading start\n",
    "        T = trading_start + pd.DateOffset(months=6) - pd.DateOffset(days=1) # 6 months after trading start - 1 day\n",
    "        max_window = len(prices.loc[t: test_end].index)\n",
    "        print(test_end.date(), len(period_candidates))\n",
    "\n",
    "        def process_pair(i, y, x):\n",
    "            pair = prices.loc[t: T, [y, x]]\n",
    "            hedge_ratio, intercept = KFHedgeRatio(y=KFSmoother(prices.loc[t: T, y]),\n",
    "                                                  x=KFSmoother(prices.loc[t: T, x]))\n",
    "            \n",
    "            pair['hedge_ratio'] = hedge_ratio\n",
    "            pair['intercept'] = intercept\n",
    "            pair['spread'] = pair[y].add(pair[x].mul(pair['hedge_ratio'])).add(pair['intercept'])\n",
    "            half_life = estimate_half_life(pair.spread.loc[t: test_end])                \n",
    "\n",
    "            spread = pair.spread.rolling(window=min(2 * half_life, max_window))\n",
    "            pair['z_score'] = pair.spread.sub(spread.mean()).div(spread.std())\n",
    "            pair_out = pair.loc[trading_start: T].assign(s1=y, s2=x, period=p, pair=i).drop([x, y], axis=1)\n",
    "\n",
    "            hl_out = [test_end, y, x, half_life]\n",
    "            return pair_out, hl_out\n",
    "\n",
    "        pair_results = Parallel(n_jobs=-1, verbose=10)(\n",
    "            delayed(process_pair)(i, y, x)\n",
    "            for i, (y, x) in enumerate(zip(period_candidates.y, period_candidates.x), 1)\n",
    "        )\n",
    "\n",
    "        pairs.extend([pr[0] for pr in pair_results])\n",
    "        half_lives.extend([pr[1] for pr in pair_results])\n",
    "\n",
    "    return pairs, half_lives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, half_lives = get_spread_parallel(candidates, smoothed_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Half Lives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:18:38.449198Z",
     "start_time": "2020-06-19T16:18:38.420580Z"
    }
   },
   "outputs": [],
   "source": [
    "hl = pd.DataFrame(half_lives, columns=['test_end', 's1', 's2', 'half_life'])\n",
    "hl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:18:38.460654Z",
     "start_time": "2020-06-19T16:18:38.450071Z"
    }
   },
   "outputs": [],
   "source": [
    "hl.half_life.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:18:38.488001Z",
     "start_time": "2020-06-19T16:18:38.461724Z"
    }
   },
   "outputs": [],
   "source": [
    "hl.to_hdf('backtest.h5', 'half_lives')\n",
    "hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hl.half_life.describe())\n",
    "\n",
    "import plotly.express as px\n",
    "fig = px.histogram(hl.half_life)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:18:42.640734Z",
     "start_time": "2020-06-19T16:18:38.489283Z"
    }
   },
   "outputs": [],
   "source": [
    "pair_data = pd.concat(pairs)\n",
    "pair_data.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:18:43.195991Z",
     "start_time": "2020-06-19T16:18:42.641767Z"
    }
   },
   "outputs": [],
   "source": [
    "pair_data.to_hdf('backtest.h5', 'pair_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:18:43.656592Z",
     "start_time": "2020-06-19T16:18:43.196963Z"
    }
   },
   "outputs": [],
   "source": [
    "pair_data = pd.read_hdf('backtest.h5', 'pair_data')\n",
    "pair_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots the price series of a cointegrated pair (s1 and s2) for a given period and pair ID.\n",
    "    \n",
    "This function loads the necessary price data and ticker names from 'backtest.h5'.\n",
    "It filters the pair_data for the specified period and pair_id to identify s1 and s2,\n",
    "then plots their close prices over the trading dates in that period.\n",
    "\n",
    "Parameters:\n",
    "- period (int): The period number (e.g., 1, 2, ..., corresponding to quarterly test periods).\n",
    "- pair_id (int): The pair identifier within the period (e.g., 1, 2, ... for each candidate pair).\n",
    "- normalize (bool, optional): If True, normalize prices to start at 100 for easier comparison. Default is False.\n",
    "- figsize (tuple, optional): Figure size for the plot. Default is (12, 6).\n",
    "\n",
    "Returns:\n",
    "- None: Displays the plot.\n",
    "\n",
    "Explanation:\n",
    "In statistical arbitrage with cointegrated pairs, visualizing the raw price series of the two assets (e.g., an ETF and a stock)\n",
    "helps understand their co-movement. Cointegrated pairs tend to move together in the long run, even if they diverge temporarily.\n",
    "- The left y-axis shows the price of the first asset (s1).\n",
    "- The right y-axis shows the price of the second asset (s2) for better scaling.\n",
    "- If normalize=True, prices are rebased to 100 at the start of the period to highlight relative movements, which is useful for spotting divergences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_pair_prices(period, pair_id, normalize=False, figsize=(12, 8)):\n",
    "    # Load prices, pair_data, and ticker names from the HDF store\n",
    "    with pd.HDFStore('backtest.h5') as store:\n",
    "        prices = store['prices'].close.unstack('ticker')  # Close prices with dates as index, tickers as columns\n",
    "        pair_data = store['pair_data']  # Load pair_data which contains hedge_ratio, intercept, and spread\n",
    "        tickers = store['tickers'].to_dict()  # Dictionary mapping ticker symbols to names\n",
    "\n",
    "    # Filter pair_data for the given period and pair_id\n",
    "    data = pair_data.query('period == @period & pair == @pair_id')\n",
    "    \n",
    "    if data.empty:\n",
    "        print(f\"No data found for period {period} and pair {pair_id}.\")\n",
    "        return\n",
    "    \n",
    "    # Extract s1 and s2 (the pair tickers)\n",
    "    s1 = data['s1'].iloc[0]  # Dependent variable (y)\n",
    "    s2 = data['s2'].iloc[0]  # Independent variable (x)\n",
    "    \n",
    "    # Get the date range for this period/pair (trading dates)\n",
    "    dates = data.index\n",
    "    \n",
    "    # Extract prices for s1 and s2 over the date range\n",
    "    pair_prices = prices.loc[dates, [s1, s2]].dropna()  # Drop any NaNs if present\n",
    "    \n",
    "    if pair_prices.empty:\n",
    "        print(f\"No price data available for {s1} and {s2} in the given period.\")\n",
    "        return\n",
    "    \n",
    "    # Estimate the price of s1 using hedge_ratio and intercept\n",
    "    # Since spread = s1 + hedge_ratio * s2 + intercept ≈ 0,\n",
    "    # estimated_s1 ≈ -hedge_ratio * s2 - intercept\n",
    "    estimated = -data['hedge_ratio'] * pair_prices[s2] - data['intercept']\n",
    "    \n",
    "    # Add estimated prices and spread to the DataFrame\n",
    "    pair_prices['estimated'] = estimated\n",
    "    pair_prices['spread'] = data['spread']\n",
    "    \n",
    "    # Optionally normalize prices to start at 100\n",
    "    if normalize:\n",
    "        pair_prices[[s1, s2, 'estimated']] = (pair_prices[[s1, s2, 'estimated']] / pair_prices[[s1, s2, 'estimated']].iloc[0]) * 100\n",
    "    \n",
    "    # Rename columns to use full names for the legend\n",
    "    pair_prices = pair_prices.rename(columns={\n",
    "        s1: tickers.get(s1, s1),\n",
    "        s2: tickers.get(s2, s2),\n",
    "        'estimated': f\"Estimated {tickers.get(s1, s1)}\"\n",
    "    })\n",
    "    \n",
    "    # Create the figure with two subplots (stacked vertically)\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=figsize, sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    # Plot s1, estimated s1, and s2 on the first subplot\n",
    "    pair_prices[[pair_prices.columns[0], pair_prices.columns[2]]].plot(\n",
    "        ax=ax1,\n",
    "        style=['-', '--'],  # Solid for s1, dashed for estimated\n",
    "        legend=True\n",
    "    )\n",
    "    pair_prices[pair_prices.columns[1]].plot(\n",
    "        ax=ax1,\n",
    "        secondary_y=True,  # s2 on secondary y-axis\n",
    "        style='-',  # Solid for s2\n",
    "        legend=True\n",
    "    )\n",
    "    \n",
    "    # Customize the first subplot\n",
    "    ax1.set_title(f\"Price Series for Pair {pair_id} in Period {period}: {pair_prices.columns[0]} vs {pair_prices.columns[1]} (with Estimated {pair_prices.columns[0]})\")\n",
    "    ax1.set_ylabel(\"Price\" if not normalize else \"Normalized Price (starting at 100)\")\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot the spread on the second subplot\n",
    "    pair_prices['spread'].plot(ax=ax2, color='purple', legend=True)\n",
    "    ax2.axhline(0, color='black', linestyle='--', linewidth=1)  # Add horizontal line at zero\n",
    "    ax2.set_title(f\"Spread for Pair {pair_id} in Period {period}\")\n",
    "    ax2.set_xlabel(\"Date\")\n",
    "    ax2.set_ylabel(\"Spread\")\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Apply Seaborn despine for cleaner look\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: Plot for period 1, pair 1 (adjust based on your data)\n",
    "plot_pair_prices(period=1, pair_id=1, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Long & Short Entry and Exit Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_trades(data):\n",
    "#     pair_trades = []\n",
    "#     for i, ((period, s1, s2), pair) in enumerate(data.groupby(['period', 's1', 's2']), 1):\n",
    "#         if i % 100 == 0:\n",
    "#             print(i)\n",
    "\n",
    "#         first3m = pair.first('3M').index\n",
    "#         last3m = pair.last('3M').index\n",
    "\n",
    "#         entry = pair.z_score.abs() > 2\n",
    "#         entry = ((entry.shift() != entry)\n",
    "#                  .mul(np.sign(pair.z_score))\n",
    "#                  .fillna(0)\n",
    "#                  .astype(int)\n",
    "#                  .sub(2))\n",
    "\n",
    "#         exit = (np.sign(pair.z_score.shift().fillna(method='bfill'))\n",
    "#                 != np.sign(pair.z_score)).astype(int) - 1\n",
    "\n",
    "#         trades = (entry[entry != -2].append(exit[exit == 0])\n",
    "#                   .to_frame('side')\n",
    "#                   .sort_values(['date', 'side'])\n",
    "#                   .squeeze())\n",
    "#         if not isinstance(trades, pd.Series):\n",
    "#             continue\n",
    "#         try:\n",
    "#             trades.loc[trades < 0] += 2\n",
    "#         except:\n",
    "#             print(type(trades))\n",
    "#             print(trades)\n",
    "#             print(pair.z_score.describe())\n",
    "#             break\n",
    "\n",
    "#         trades = trades[trades.abs().shift() != trades.abs()]\n",
    "#         window = trades.loc[first3m.min():first3m.max()]\n",
    "#         extra = trades.loc[last3m.min():last3m.max()]\n",
    "#         n = len(trades)\n",
    "\n",
    "#         if window.iloc[0] == 0:\n",
    "#             if n > 1:\n",
    "#                 print('shift')\n",
    "#                 window = window.iloc[1:]\n",
    "#         if window.iloc[-1] != 0:\n",
    "#             extra_exits = extra[extra == 0].head(1)\n",
    "#             if extra_exits.empty:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 window = window.append(extra_exits)\n",
    "\n",
    "#         trades = pair[['s1', 's2', 'hedge_ratio', 'period', 'pair']].join(window.to_frame('side'), how='right')\n",
    "#         trades.loc[trades.side == 0, 'hedge_ratio'] = np.nan\n",
    "#         trades.hedge_ratio = trades.hedge_ratio.ffill()\n",
    "#         pair_trades.append(trades)\n",
    "#     return pair_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:18:43.665632Z",
     "start_time": "2020-06-19T16:18:43.657454Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_trades(data):\n",
    "    pair_trades = []\n",
    "    for i, ((period, s1, s2), pair) in enumerate(data.groupby(['period', 's1', 's2']), 1):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "\n",
    "        first3m = pair.first('3M').index\n",
    "        last3m = pair.last('3M').index\n",
    "\n",
    "        entry = pair.z_score.abs() > 2\n",
    "        entry = ((entry.shift() != entry)\n",
    "                 .mul(np.sign(pair.z_score))\n",
    "                 .fillna(0)\n",
    "                 .astype(int)\n",
    "                 .sub(2))\n",
    "\n",
    "        exit = (np.sign(pair.z_score.shift().fillna(method='bfill'))\n",
    "                != np.sign(pair.z_score)).astype(int) - 1\n",
    "\n",
    "        trades = (pd.concat([entry[entry != -2], exit[exit == 0]])\n",
    "                  .to_frame('side')\n",
    "                  .reset_index()  # Reset index to make 'date' a column for sorting\n",
    "                  .sort_values(['date', 'side'])\n",
    "                  .set_index('date')  # Set 'date' back as index\n",
    "                  .squeeze())\n",
    "\n",
    "        if not isinstance(trades, pd.Series):\n",
    "            continue\n",
    "        try:\n",
    "            trades.loc[trades < 0] += 2\n",
    "        except:\n",
    "            print(type(trades))\n",
    "            print(trades)\n",
    "            print(pair.z_score.describe())\n",
    "            break\n",
    "\n",
    "        trades = trades[trades.abs().shift() != trades.abs()]\n",
    "        window = trades.loc[first3m.min():first3m.max()]\n",
    "        extra = trades.loc[last3m.min():last3m.max()]\n",
    "        n = len(trades)\n",
    "\n",
    "        if window.iloc[0] == 0:\n",
    "            if n > 1:\n",
    "                print('shift')\n",
    "                window = window.iloc[1:]\n",
    "        if window.iloc[-1] != 0:\n",
    "            extra_exits = extra[extra == 0].head(1)\n",
    "            if extra_exits.empty:\n",
    "                continue\n",
    "            else:\n",
    "                window = pd.concat([window, extra_exits])\n",
    "\n",
    "        trades = pair[['s1', 's2', 'hedge_ratio', 'period', 'pair']].join(window.to_frame('side'), how='right')\n",
    "        trades.loc[trades.side == 0, 'hedge_ratio'] = np.nan\n",
    "        trades.hedge_ratio = trades.hedge_ratio.ffill()\n",
    "        pair_trades.append(trades)\n",
    "    return pair_trades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:23:57.066089Z",
     "start_time": "2020-06-19T16:18:43.666577Z"
    }
   },
   "outputs": [],
   "source": [
    "pair_trades = get_trades(pair_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:24:00.948377Z",
     "start_time": "2020-06-19T16:23:57.066963Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pair_trade_data = pd.concat(pair_trades)\n",
    "pair_trade_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:24:00.959508Z",
     "start_time": "2020-06-19T16:24:00.949698Z"
    }
   },
   "outputs": [],
   "source": [
    "pair_trade_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:34:09.171268Z",
     "start_time": "2020-06-19T16:34:08.884111Z"
    }
   },
   "outputs": [],
   "source": [
    "trades = pair_trade_data['side'].copy()\n",
    "trades.loc[trades != 0] = 1\n",
    "trades.loc[trades == 0] = -1\n",
    "trades.sort_index().cumsum().plot(figsize=(14, 4))\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T16:24:01.303828Z",
     "start_time": "2020-06-19T16:24:01.269001Z"
    }
   },
   "outputs": [],
   "source": [
    "pair_trade_data.to_hdf('backtest.h5', 'pair_trades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
