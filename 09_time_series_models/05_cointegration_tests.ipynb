{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fastcluster\n",
    "# %pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cointegration Tests & Pairs Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is cointegration?\n",
    "We have seen how a time series can have a unit root that creates a stochastic trend and makes the time series highly persistent. When we use such an integrated time series in their original, rather than in differenced, form as a feature in a linear regression model, its relationship with the outcome will often appear statistically significant, even though it is not. This phenomenon is called spurious regression (for details, see Chapter 18 in [Wooldridge, 2008](https://economics.ut.ac.ir/documents/3030266/14100645/Jeffrey_M._Wooldridge_Introductory_Econometrics_A_Modern_Approach__2012.pdf)). Therefore, the recommended solution is to difference the time series so they become stationary before using them in a model.\n",
    "\n",
    "However, there is an exception when there are cointegration relationships between the outcome and one or more input variables. To understand the concept of cointegration, let's first remember that the residuals of a regression model are a linear combination of the inputs and the output series.\n",
    "\n",
    "Usually, the residuals of the regression of one integrated time series on one or more such series yields non-stationary residuals that are also integrated, and thus behave like a random walk. However, for some time series, this is not the case: the regression produces coefficients that yield a linear combination of the time series in the form of the residuals that are stationary, even though the individual series are not. Such time series are\n",
    "cointegrated.\n",
    "\n",
    "A non-technical example is that of a drunken man on a random walk accompanied by his dog (on a leash). Both trajectories are non-stationary but cointegrated because the dog will occasionally revert to his owner. In the trading context, arbitrage constraints imply cointegration between spot and futures prices.\n",
    "\n",
    "In other words, a linear combination of two or more cointegrated series has a stable mean to which this linear combination reverts. This also applies when the individual series are integrated of a higher order and the linear combination reduces the overall order of integration.\n",
    "\n",
    "Cointegration differs from correlation: two series can be highly correlated but need not be cointegrated. For example, if two growing series are constant multiples of each other, their correlation will be high, but any linear combination will also grow rather than revert to a stable mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cointegration for Pairs Trading\n",
    "\n",
    "Cointegration is very useful: if two or more asset price series tend to revert to a common mean, we can leverage deviations from the trend because they should imply future price moves in the opposite direction. The mathematics behind cointegration is more involved, so we will only focus on the practical aspects; for an in-depth treatment, see [Lütkepohl (2005)](https://www.springer.com/gp/book/9783540401728).\n",
    "\n",
    "In this notebook, we will address how we can identify pairs with such a long-term stationary relationship, estimate the expected time for any disequilibrium to correct, and how to utilize these tools to implement and backtest a long-short pairs trading strategy. There are two approaches to testing for cointegration:\n",
    "- The Engle-Granger two-step method\n",
    "- The Johansen test\n",
    "\n",
    "The book chapter discusses each test in turn; in this notebook we show how they help identify cointegrated securities that tend to revert to a common trend, a fact that we can leverage for a statistical arbitrage\n",
    "strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T14:37:17.768213Z",
     "start_time": "2021-02-23T14:37:17.765416Z"
    }
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:45.781616Z",
     "start_time": "2021-02-23T15:13:45.765250Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import LinAlgError\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import  DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, coint\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T14:37:18.698099Z",
     "start_time": "2021-02-23T14:37:18.695965Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T14:37:18.705831Z",
     "start_time": "2021-02-23T14:37:18.699717Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = Path('..', 'data')\n",
    "STORE = DATA_PATH / 'assets.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Johansen Test Critical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These critical values for the Johansen trace statistic are sourced from the tables in Osterwald-Lenum (1992), \"A Note with Quantiles of the Asymptotic Distribution of the Maximum Likelihood Cointegration Rank Test Statistics,\" published in the Oxford Bulletin of Economics and Statistics (Volume 54, Issue 3, pages 461-472). They correspond to the case with a constant term but no linear trend (det_order=0 in statsmodels), for a bivariate system (two variables), at the 90%, 95%, and 99% significance levels:\n",
    "\n",
    "- For testing 0 cointegration relationships (rank 0): 13.4294 (90%), 15.4943 (95%), 19.9349 (99%).\n",
    "- For testing 1 cointegration relationship (rank 1): 2.7055 (90%), 3.8415 (95%), 6.6349 (99%).\n",
    "\n",
    "These values are commonly hardcoded or referenced in implementations like statsmodels' `coint_johansen` function, which draws from this paper for systems with fewer variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T14:37:18.713695Z",
     "start_time": "2021-02-23T14:37:18.707298Z"
    }
   },
   "outputs": [],
   "source": [
    "critical_values = {0: {.9: 13.4294, .95: 15.4943, .99: 19.9349},\n",
    "                   1: {.9: 2.7055, .95: 3.8415, .99: 6.6349}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T14:37:18.721417Z",
     "start_time": "2021-02-23T14:37:18.714576Z"
    }
   },
   "outputs": [],
   "source": [
    "trace0_cv = critical_values[0][.95] # critical value for 0 cointegration relationships\n",
    "trace1_cv = critical_values[1][.95] # critical value for 1 cointegration relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Clean Stock & ETF Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove highly correlated assets\n",
    "\n",
    "A function removes highly correlated assets (correlation > 0.99) to avoid redundancy:\n",
    "\n",
    "It computes the correlation matrix, identifies pairs above the cutoff, and decides which to keep/drop to minimize duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T14:37:18.733836Z",
     "start_time": "2021-02-23T14:37:18.722277Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_correlated_assets(df, cutoff=.99):\n",
    "    corr = df.corr().stack()\n",
    "    corr = corr[corr < 1]\n",
    "    to_check = corr[corr.abs() > cutoff].index\n",
    "    keep, drop = set(), set()\n",
    "    for s1, s2 in to_check:\n",
    "        if s1 not in keep:\n",
    "            if s2 not in keep:\n",
    "                keep.add(s1)\n",
    "                drop.add(s2)\n",
    "            else:\n",
    "                drop.add(s1)\n",
    "        else:\n",
    "            keep.discard(s2)\n",
    "            drop.add(s2)\n",
    "    return df.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stationary series\n",
    "\n",
    "It runs ADF on each series (with constant and trend) and collects p-values.\n",
    "\n",
    "A related function removes stationary series (ADF p-value <= 0.05):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T14:37:18.745120Z",
     "start_time": "2021-02-23T14:37:18.734706Z"
    }
   },
   "outputs": [],
   "source": [
    "# def check_stationarity(df):\n",
    "#     results = []\n",
    "#     for ticker, prices in df.items():\n",
    "#         results.append([ticker, adfuller(prices, regression='ct')[1]])\n",
    "#     return pd.DataFrame(results, columns=['ticker', 'adf']).sort_values('adf')\n",
    "\n",
    "def check_stationarity(df):\n",
    "    def run_adf(ticker, prices):\n",
    "        return [ticker, adfuller(prices, regression='ct')[1]]\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(run_adf)(ticker, prices) for ticker, prices in df.items()\n",
    "    )\n",
    "    return pd.DataFrame(results, columns=['ticker', 'adf']).sort_values('adf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T14:37:18.753562Z",
     "start_time": "2021-02-23T14:37:18.747783Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stationary_assets(df, pval=.05):\n",
    "    test_result = check_stationarity(df)\n",
    "    stationary = test_result.loc[test_result.adf <= pval, 'ticker'].tolist()\n",
    "    return df.drop(stationary, axis=1).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/engineered_features', '/us_equities/stocks', '/stooq/us/nysemkt/stocks/prices', '/stooq/us/nysemkt/stocks/tickers', '/stooq/us/nyse/stocks/prices', '/stooq/us/nyse/stocks/tickers', '/stooq/us/nyse/etfs/prices', '/stooq/us/nyse/etfs/tickers', '/stooq/us/nasdaq/stocks/prices', '/stooq/us/nasdaq/stocks/tickers', '/stooq/us/nasdaq/etfs/prices', '/stooq/us/nasdaq/etfs/tickers', '/stooq/jp/tse/stocks/prices', '/stooq/jp/tse/stocks/tickers', '/sp500/fred', '/sp500/stocks', '/sp500/stooq', '/quandl/wiki/prices', '/quandl/wiki/stocks']\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(STORE) as store:\n",
    "    print(store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T14:58:37.683658Z",
     "start_time": "2021-02-23T14:58:37.677577Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_assets(asset_class='stocks', n=500, start=2010, end=2019):\n",
    "    idx = pd.IndexSlice\n",
    "    with pd.HDFStore(STORE) as store:\n",
    "        df = (pd.concat([store[f'stooq/us/nasdaq/{asset_class}/prices'],\n",
    "                         store[f'stooq/us/nyse/{asset_class}/prices']])\n",
    "              # stooq download can have duplicate assets\n",
    "              .loc[lambda df: ~df.index.duplicated()]\n",
    "              .sort_index()\n",
    "              .loc[idx[:, f'{start}':f'{end}'], :]\n",
    "              .assign(dv=lambda df: df.close.mul(df.volume)))\n",
    "\n",
    "    # select n assets with the highest average trading volume\n",
    "    # we are taking a shortcut to simplify; should select\n",
    "    # based on historical only, e.g. yearly rolling avg\n",
    "    most_traded = (df.groupby(level='ticker')\n",
    "                   .dv.mean()\n",
    "                   .nlargest(n=n).index)\n",
    "\n",
    "    df = (df.loc[idx[most_traded, :], 'close']\n",
    "          .unstack('ticker')\n",
    "          .ffill(limit=5)  # fill up to five values\n",
    "          .dropna(axis=1))  # remove assets with any missing values\n",
    "\n",
    "    df = remove_correlated_assets(df)\n",
    "    return remove_stationary_assets(df).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/engineered_features', '/us_equities/stocks', '/stooq/us/nysemkt/stocks/prices', '/stooq/us/nysemkt/stocks/tickers', '/stooq/us/nyse/stocks/prices', '/stooq/us/nyse/stocks/tickers', '/stooq/us/nyse/etfs/prices', '/stooq/us/nyse/etfs/tickers', '/stooq/us/nasdaq/stocks/prices', '/stooq/us/nasdaq/stocks/tickers', '/stooq/us/nasdaq/etfs/prices', '/stooq/us/nasdaq/etfs/tickers', '/stooq/jp/tse/stocks/prices', '/stooq/jp/tse/stocks/tickers', '/sp500/fred', '/sp500/stocks', '/sp500/stooq', '/quandl/wiki/prices', '/quandl/wiki/stocks']\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(STORE) as store:\n",
    "    print(store.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the intermediate result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T14:59:45.554530Z",
     "start_time": "2021-02-23T14:58:39.150056Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/ycbntly138n0xkt0fdrnnk_w0000gn/T/ipykernel_47105/4225768641.py:3: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  df.to_hdf('data.h5', f'{asset_class}/close')\n"
     ]
    }
   ],
   "source": [
    "for asset_class, n in [('etfs', 500), ('stocks', 250)]:\n",
    "    df = select_assets(asset_class=asset_class, n=n)\n",
    "    df.to_hdf('data.h5', f'{asset_class}/close')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ticker dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T14:59:57.148863Z",
     "start_time": "2021-02-23T14:59:57.145495Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ticker_dict():\n",
    "    with pd.HDFStore(STORE) as store:\n",
    "        return (pd.concat([\n",
    "            store['stooq/us/nyse/stocks/tickers'],\n",
    "            store['stooq/us/nyse/etfs/tickers'],\n",
    "            store['stooq/us/nasdaq/etfs/tickers'],\n",
    "            store['stooq/us/nasdaq/stocks/tickers']\n",
    "        ]).drop_duplicates().set_index('ticker').squeeze().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T14:59:57.691206Z",
     "start_time": "2021-02-23T14:59:57.628201Z"
    }
   },
   "outputs": [],
   "source": [
    "names = get_ticker_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Correlation Clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload intermediate results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:00:54.264967Z",
     "start_time": "2021-02-23T15:00:54.248699Z"
    }
   },
   "outputs": [],
   "source": [
    "stocks = pd.read_hdf('data.h5', 'stocks/close')\n",
    "stocks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:00:56.705358Z",
     "start_time": "2021-02-23T15:00:56.690013Z"
    }
   },
   "outputs": [],
   "source": [
    "etfs = pd.read_hdf('data.h5', 'etfs/close')\n",
    "etfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:01:15.101618Z",
     "start_time": "2021-02-23T15:01:13.647425Z"
    }
   },
   "outputs": [],
   "source": [
    "tickers = {k: v for k, v in names.items() if k in etfs.columns.union(stocks.columns)}\n",
    "pd.Series(tickers).to_hdf('data.h5', 'tickers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlations in the matrix (`corr`) are computed using the Pearson correlation coefficient (the default for pandas' `.corrwith()` method) between the time series of closing prices for each stock (columns in `stocks`) and each ETF (columns in `etfs`). This measures the linear relationship between the raw price levels over the shared time period (2010-2019 in the dataset), without any transformations like differencing or normalization applied at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:01:19.874810Z",
     "start_time": "2021-02-23T15:01:15.102722Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_list = []\n",
    "for etf, data in etfs.items():\n",
    "    corr_list.append(stocks.corrwith(data).rename(etf))\n",
    "corr = pd.concat(corr_list, axis=1)\n",
    "corr.index = stocks.columns  # Ensure index is stock tickers if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:01:19.888310Z",
     "start_time": "2021-02-23T15:01:19.875906Z"
    }
   },
   "outputs": [],
   "source": [
    "corr.info()\n",
    "print(corr.shape)\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustermap generated by this code visualizes a correlation matrix (`corr`) as a heatmap with added hierarchical clustering, making it easier to spot patterns, groups, and relationships in the data. Here's a step-by-step guide to reading and interpreting it:\n",
    "\n",
    "### 1. **Understand the Overall Structure**\n",
    "   - **Central Heatmap**: This is the main grid of colored cells. Each row represents a stock (from the index of `corr`), and each column represents an ETF (from the columns of `corr`). The color of each cell shows the correlation value between that specific stock and ETF.\n",
    "     - Correlations range from -1 (strong negative) to +1 (strong positive).\n",
    "     - The map is not in the original order of `corr`; rows and columns are reordered based on clustering to group similar items together.\n",
    "   - **Dendrograms (Tree-like Structures)**:\n",
    "     - **Left Side**: Shows the hierarchical clustering of rows (stocks). It illustrates how stocks are grouped based on the similarity of their correlation patterns with all ETFs.\n",
    "     - **Top Side**: Shows the hierarchical clustering of columns (ETFs). It groups ETFs based on how similarly they correlate with all stocks.\n",
    "     - These are like upside-down trees: branches represent merges of similar groups, with the height of each merge indicating dissimilarity (taller branches mean less similar items merged later).\n",
    "   - **No Colorbar by Default**: Seaborn often adds a colorbar legend on the right, showing the mapping from colors to numerical values (e.g., -1 to +1). If it's missing in your output, you can add `fig.colorbar()` in code to include it.\n",
    "\n",
    "### 2. **Interpret the Colors (Heatmap Values)**\n",
    "   - The colormap (`cmap`) is a diverging palette created with `sns.diverging_palette(220, 10, as_cmap=True)`:\n",
    "     - Hue 220 is typically blue/cool tones for negative values.\n",
    "     - Hue 10 is typically red/warm tones for positive values.\n",
    "     - `center=0` sets the midpoint to neutral (likely white or light gray), so:\n",
    "       - **Blue shades**: Negative correlations (e.g., dark blue for ≈ -1, meaning assets move in opposite directions).\n",
    "       - **Red shades**: Positive correlations (e.g., dark red for ≈ +1, meaning assets move together).\n",
    "       - **White/Light Gray**: Near-zero correlations (no strong relationship).\n",
    "     - Intensity/depth of color indicates strength: darker/more saturated = stronger correlation (closer to ±1); lighter = weaker (closer to 0).\n",
    "   - Scan for patterns: Blocks of similar colors suggest clusters where groups of stocks and ETFs have consistent correlation behaviors (e.g., a red block means a group of positively correlated assets).\n",
    "\n",
    "### 3. **Read the Clustering (Dendrograms)**\n",
    "   - **How Clustering Works**: The map uses hierarchical clustering (default: average linkage with Euclidean distance). It measures dissimilarity between rows/columns based on their vectors of correlation values, then groups them step-by-step.\n",
    "     - Short branches/low merge points: Highly similar items (e.g., stocks with nearly identical correlation profiles to ETFs).\n",
    "     - Long branches/high merge points: More dissimilar items, merged later.\n",
    "   - **Identify Clusters**:\n",
    "     - Trace from the leaves (individual labels) up the dendrogram to find merge points. A \"cluster\" is a subtree where items merge early (low height).\n",
    "     - For example, if several stocks merge into a small branch on the left dendrogram, they form a cluster—meaning those stocks have similar correlation patterns across all ETFs (e.g., they might all be tech stocks correlating highly with tech ETFs).\n",
    "     - Similarly for ETFs on top: Clustered ETFs might represent similar sectors (e.g., energy ETFs grouping together).\n",
    "     - The reordering aligns similar clusters near each other in the heatmap, creating visible \"blocks\" of high/low correlations.\n",
    "   - **Cluster Size and Hierarchy**: Larger clusters (many leaves under one high branch) indicate broad similarities; smaller ones show niche groups. You can mentally \"cut\" the dendrogram at a certain height to define cluster boundaries (e.g., cut low for many small clusters, high for fewer large ones).\n",
    "\n",
    "### 4. **Spot Insights and Patterns**\n",
    "   - **Similar Assets**: Look for dense red blocks—these highlight groups of stocks and ETFs that move together (potential for sector analysis or diversification risks).\n",
    "   - **Opposites**: Blue blocks indicate inverse relationships (useful for hedging in trading).\n",
    "   - **Outliers**: Isolated branches or rows/columns with unique color patterns suggest assets that don't fit well with others.\n",
    "   - **Overall Trends**: If the map shows mostly red, correlations are generally positive (common in markets). Fragmented clusters might indicate diverse behaviors.\n",
    "   - This visualization helps identify \"clusters of similar assets\" as noted—e.g., stocks clustering with certain ETFs might share industries or risk factors.\n",
    "\n",
    "### 5. **Tips for Interaction/Exploration**\n",
    "   - **Zoom In**: If the plot is crowded (many stocks/ETFs), hover over cells in an interactive environment (e.g., Jupyter) for exact values, or subset `corr` before plotting.\n",
    "   - **Customize for Clarity**: To make it easier to read, you could modify the code:\n",
    "     - Add labels: `sns.clustermap(corr, cmap=cmap, center=0, xticklabels=True, yticklabels=True)`\n",
    "     - Change clustering: `method='single'` or `metric='correlation'` for different groupings.\n",
    "     - Save/Export: `fig = sns.clustermap(...); fig.savefig('clustermap.png')`\n",
    "   - **Limitations**: Clustering is sensitive to parameters; it's exploratory, not definitive. Correlations don't imply causation, and results depend on the data period.\n",
    "\n",
    "If the plot isn't displaying or you need a specific example from running the code, provide more details about your environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:01:20.585582Z",
     "start_time": "2021-02-23T15:01:19.889466Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt  # Ensure this is imported\n",
    "\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "fig = sns.clustermap(corr, cmap=cmap, center=0, xticklabels=True, yticklabels=True, figsize=(20, 20))  # Keep larger size for visibility\n",
    "\n",
    "# Make x and y tick labels smaller (adjust labelsize as needed, e.g., 6 for very small)\n",
    "fig.ax_heatmap.tick_params(axis='x', labelsize=8)\n",
    "fig.ax_heatmap.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "# Optional: Rotate x labels for better fit if still crowded\n",
    "fig.ax_heatmap.set_xticklabels(fig.ax_heatmap.get_xticklabels(), rotation=90)\n",
    "\n",
    "# Save if needed for zooming\n",
    "fig.savefig('clustermap_small_labels.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Selection using Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity: Comparing running times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we compare the running times of various cointegration tests. More specifically, we are running tests for a single asset vs. the remaining set of securities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:01:38.714843Z",
     "start_time": "2021-02-23T15:01:38.696321Z"
    }
   },
   "outputs": [],
   "source": [
    "stocks.info()\n",
    "print(stocks.shape)\n",
    "stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:01:38.732329Z",
     "start_time": "2021-02-23T15:01:38.716060Z"
    }
   },
   "outputs": [],
   "source": [
    "etfs.info()\n",
    "print(etfs.shape)\n",
    "etfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etfs.iloc[:,].plot(figsize=(20, 6))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2011-02-17  data corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:01:38.743544Z",
     "start_time": "2021-02-23T15:01:38.733468Z"
    }
   },
   "outputs": [],
   "source": [
    "security = etfs['AAXJ.US'].loc['2012': '2015']\n",
    "candidates = stocks.loc['2012': '2015']\n",
    "# candidates = stocks.loc['2011-02-17': '2011-02-28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "security.plot(figsize=(20, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Normalization of Price Series\n",
    "```python\n",
    "security = security.div(security.iloc[0])\n",
    "candidates = candidates.div(candidates.iloc[0])\n",
    "```\n",
    "- **What it does**: Normalizes both the `security` (e.g., ETF prices) and `candidates` (e.g., stock prices) by dividing each series by its first value (`iloc[0]`). This scales all series to start at 1, making them comparable regardless of absolute price levels.\n",
    "- **Why?**: In pairs trading, we care about relative movements, not absolute prices. Normalization helps compute meaningful spreads and correlations (e.g., avoiding bias from high-priced vs. low-priced assets).\n",
    "- **Details**: `security` is a Series (one asset's prices over time). `candidates` is a DataFrame (multiple assets' prices, same time index). `ticker` extracts the name (e.g., ETF ticker) for later use in results.\n",
    "- **Context from markdown**: This aligns with the \"distance approach,\" where normalized prices are used to compute correlations or spreads for identifying comoving pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:01:38.749550Z",
     "start_time": "2021-02-23T15:01:38.744629Z"
    }
   },
   "outputs": [],
   "source": [
    "print(security.iloc[0])\n",
    "print(security)\n",
    "security = security.div(security.iloc[0])\n",
    "print(\"After scales all series to start at 1, making them comparable regardless of absolute price levels.\")\n",
    "print(security.describe())\n",
    "security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates.iloc[0]\n",
    "candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = candidates.div(candidates.iloc[0])\n",
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute Spreads\n",
    "\n",
    "**Purpose:** \n",
    "- In pairs trading, the spread represents the relative deviation between two assets. For cointegrated pairs, the spread should be stationary (mean-reverting) rather than trending or random-walking. Low drift and volatility in the spread are heuristics for potential cointegration.\n",
    "\n",
    "**How to read/understand the result:**\n",
    "\n",
    "- Print spreads to see the DataFrame: Index is dates, columns are stock tickers, values are differences (e.g., 0.05 means the stock is 5% above the security at that date, assuming normalization).\n",
    "- If the spread hovers around zero with small fluctuations, it suggests mean-reversion (good for trading). If it trends up/down, it has drift (less desirable).\n",
    "- Example: If a spread starts near 0 (due to normalization) and ends at 0.2, the pair diverged; if it oscillates and returns to 0, it's potentially cointegrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreads = candidates.sub(security, axis=0)\n",
    "spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spread(spreads, security, ticker_index):\n",
    "    # Create the figure\n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(10)\n",
    "    fig.set_figheight(3)\n",
    "\n",
    "    # Plot the specified column of the spreads DataFrame\n",
    "    plt.plot(spreads.index, spreads.iloc[:, ticker_index], label=f'{spreads.iloc[:, ticker_index].name}')\n",
    "\n",
    "    # Draw a horizontal line at y=0\n",
    "    plt.axhline(y=0, color='red', linestyle='--', label='y=0')\n",
    "\n",
    "    # Add labels and legend for better visualization\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'{spreads.iloc[:, ticker_index].name} ({ticker_index}) Spread from Security {security.name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "mean_reverting_list = [14,17,27,42,77,106]\n",
    "drift_list = [1,3,23,73,98,81]\n",
    "\n",
    "# for i in range(spreads.shape[1]):\n",
    "#     plot_spread(spreads, security, i)\n",
    "\n",
    "for i in mean_reverting_list:\n",
    "    plot_spread(spreads, security, i)\n",
    "    \n",
    "for i in drift_list:\n",
    "    plot_spread(spreads, security, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = spreads.shape\n",
    "print(n,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Set Up Design Matrix for Drift Calculation.\n",
    "\n",
    "- **What it is**: `X` is a numpy array (n rows x 2 columns) acting as the design matrix for ordinary least squares (OLS) regression.\n",
    "  - Column 0: All 1s (intercept term).\n",
    "  - Column 1: A time trend from 1 to n (linear sequence over the dates).\n",
    "  - `n` is the number of time points (rows in `spreads`).\n",
    "- **Purpose**: This sets up a simple linear regression model: spread_t = β0 + β1 * t + ε_t, where t is time. The slope β1 estimates the \"drift\" (average daily change in the spread). Low absolute drift indicates a stable, non-trending spread (heuristic for cointegration).\n",
    "- **Output/Result**: A 2D array like `[[1, 1], [1, 2], ..., [1, n]]`.\n",
    "- **How to read/understand the result**:\n",
    "  - This isn't directly outputted, but it's used in the timed OLS below. The full regression (not shown here but in the full code) extracts β1 as drift for each spread column.\n",
    "  - Interpret drift: If β1 ≈ 0, the spread is stable (good). If β1 = 0.001, the spread widens by 0.1% per period on average (bad for mean-reversion trading).\n",
    "  - In context: Pairs with low |drift| (e.g., <0.0001) are prioritized for further testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:01:38.756941Z",
     "start_time": "2021-02-23T15:01:38.750525Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.ones(shape=(n, 2))\n",
    "X[:, 1] = np.arange(1, n+1)\n",
    "\n",
    "print(len(X))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Compute Drift\n",
    "\n",
    "Computes the ordinary least squares (OLS) regression coefficients for a simple linear regression model fitted independently to each column of the spreads DataFrame. It uses matrix algebra to solve for the coefficients in a vectorized way (efficient for multiple regressions at once, like across m candidate spreads).\n",
    "\n",
    "2. **Derivation of the OLS Estimator**:\n",
    "    - The least squares solution comes from setting the derivative of the error to zero (normal equations):  $X^T X \\beta = X^T y$ .\n",
    "    - Solve for β: Multiply both sides by the inverse of  $X^T X$ :\n",
    "\n",
    "$\\hat{\\beta} = (X^T X)^{-1} X^T y$\n",
    "\n",
    "https://www.youtube.com/watch?v=NN7mBupK-8o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:01:44.686593Z",
     "start_time": "2021-02-23T15:01:38.757885Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# np.linalg.inv(X.T @ X) @ X.T @ spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OLS coefficients\n",
    "drift_result = np.linalg.inv(X.T @ X) @ X.T @ spreads\n",
    "\n",
    "# Time the operation (mimicking %%timeit output)\n",
    "import time\n",
    "start = time.time()\n",
    "np.linalg.inv(X.T @ X) @ X.T @ spreads\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nExecution time: {elapsed*1e6:.2f} µs\")\n",
    "\n",
    "# Print result\n",
    "print(\"OLS Coefficients (β₀, β₁) for each spread:\")\n",
    "drift_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compute Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:01:53.021935Z",
     "start_time": "2021-02-23T15:01:44.688097Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# spreads.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute standard deviation\n",
    "vol_result = spreads.std()\n",
    "\n",
    "# Time the operation\n",
    "start = time.time()\n",
    "spreads.std()\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nExecution time: {elapsed*1e6:.2f} µs\")\n",
    "\n",
    "# Print result\n",
    "print(\"Standard Deviation (Volatility) of each spread:\")\n",
    "vol_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:01:55.542166Z",
     "start_time": "2021-02-23T15:01:53.023518Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# candidates.corrwith(security)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation\n",
    "corr_result = candidates.corrwith(security)\n",
    "\n",
    "# Time the operation\n",
    "start = time.time()\n",
    "candidates.corrwith(security)\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nExecution time: {elapsed*1e6:.2f} µs\")\n",
    "\n",
    "# Print result\n",
    "print(\"Correlation between security and each candidate:\")\n",
    "corr_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose drift_result to shape (n, 2) and convert to DataFrame\n",
    "drift_df = pd.DataFrame(drift_result.T.values, columns=['beta0', 'beta1'], index=vol_result.index)\n",
    "combined_df = drift_df.assign(vol=vol_result, corr=corr_result)\n",
    "combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cointegration Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2m 1.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:06:17.974043Z",
     "start_time": "2021-02-23T15:01:55.544702Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# for candidate, prices in candidates.items():\n",
    "#     df = pd.DataFrame({'s1': security,\n",
    "#                        's2': prices})\n",
    "#     var = VAR(df.values)\n",
    "#     lags = var.select_order()\n",
    "#     k_ar_diff = lags.selected_orders['aic']\n",
    "#     coint_johansen(df, det_order=0, k_ar_diff=k_ar_diff)\n",
    "#     coint(security, prices, trend='c')[:2]\n",
    "#     coint(prices, security, trend='c')[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_candidate(candidate, prices, security):\n",
    "    # Step 1: Create a DataFrame for the two series.\n",
    "    # - 's1' is the security (reference ETF), 's2' is the candidate stock.\n",
    "    # - This formats the data for vector-based models like VAR and Johansen.\n",
    "    df = pd.DataFrame({'s1': security, 's2': prices})\n",
    "    \n",
    "    # Step 2: Fit a Vector Autoregression (VAR) model on the raw values.\n",
    "    # - VAR models the joint dynamics of the two series to determine dependencies.\n",
    "    # - Uses `df.values` (NumPy array) for efficiency.\n",
    "    # - Context: VAR is a precursor to VECM for cointegration; it's used here to select lags empirically.\n",
    "    var = VAR(df.values)\n",
    "    \n",
    "    # Step 3: Select optimal lag order for the VAR model.\n",
    "    # - `select_order()` computes information criteria (AIC, BIC, etc.) for different lags.\n",
    "    # - Context: Lag selection is crucial for accurate cointegration testing; too few/many lags can bias results.\n",
    "    lags = var.select_order()\n",
    "    \n",
    "    # Step 4: Extract the AIC-selected lag order (k_ar_diff).\n",
    "    # - 'aic' is chosen as the criterion (Akaike Information Criterion balances fit and complexity).\n",
    "    # - This lag is used in the Johansen test to account for autocorrelation in residuals.\n",
    "    # - Context: In the full code, this avoids arbitrary lag choices; Johansen requires this for reliable trace statistics.\n",
    "    k_ar_diff = lags.selected_orders['aic']\n",
    "    \n",
    "    # Step 5: Run the Johansen cointegration test.\n",
    "    # - `coint_johansen(df, det_order=0, k_ar_diff=k_ar_diff)`:\n",
    "    #   - `df`: The two-series DataFrame.\n",
    "    #   - `det_order=0`: No deterministic trend (constant or linear) in the cointegrating relation (simple model).\n",
    "    #   - `k_ar_diff`: The VAR-selected lags for differencing.\n",
    "    # - Returns a `JohansenTestResult` object with eigenvalues, trace statistics, etc.\n",
    "    # - Context: Johansen is the primary test here; it can detect up to 1 cointegrating vector (for two series). Later, you'd compare trace stats to critical values (e.g., `trace0_cv` > stat rejects no cointegration). This is more robust than Engle-Granger for small samples or multiple relations.\n",
    "    cj = coint_johansen(df, det_order=0, k_ar_diff=k_ar_diff)\n",
    "    \n",
    "    # Step 6: Run Engle-Granger test with security as dependent variable.\n",
    "    # - `coint(security, prices, trend='c')`: Tests if security ~ prices + constant (trend='c' includes intercept).\n",
    "    # - Slices `[:2]` to get only the test statistic and p-value (ignores critical values).\n",
    "    # - Context: Engle-Granger is a residual-based test: Regress one on the other, then check if residuals are stationary (via ADF). Running it shows if the relationship is stronger in one direction.\n",
    "    eg1 = coint(security, prices, trend='c')[:2]\n",
    "    \n",
    "    # Step 7: Run Engle-Granger test with candidate as dependent variable.\n",
    "    # - Same as above, but reversed: prices ~ security + constant.\n",
    "    # - Context: Cointegration should hold regardless of order, but asymmetry can indicate lead-lag effects (useful for trading signals).\n",
    "    eg2 = coint(prices, security, trend='c')[:2]\n",
    "    \n",
    "    # Step 8: Return results as a tuple.\n",
    "    # - Includes the candidate ticker for identification in parallel results.\n",
    "    # - Context: These are aggregated in `results_df` for post-processing (e.g., filter p-values < 0.05 or trace > critical values).\n",
    "    return candidate, cj, eg1, eg2\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_candidate)(candidate, prices, security) for candidate, prices in candidates.items()\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['candidate', 'johansen', 'eg1', 'eg2']).set_index('candidate')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, cointegration tests are significantly more costly. It would be great if the heuristics worked just as well, or at least 'good enough'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `compute_pair_metrics()` computes the following distance metrics for over 23,000 pairs of\n",
    "stocks and Exchange Traded Funds (ETFs) for 2010-14 and 2015-19:\n",
    "\n",
    "- The **drift of the spread**, defined as a linear regression of a time trend on the spread\n",
    "- The spread's  **volatility**\n",
    "- The **correlations** between the normalized price series and between their returns\n",
    "\n",
    "Low drift and volatility, as well as high correlation, are simple proxies for cointegration. \n",
    "\n",
    "To evaluate the predictive power of these heuristics, we also run Engle-Granger and Johansen **cointegration tests** using `statsmodels` for the preceding pairs. This takes place in the loop in the second half of `compute_pair_metrics()`.\n",
    "\n",
    "We first estimate the optimal number of lags that we need to specify for the Johansen test. For both tests, we assume that the cointegrated series (the spread) may have an intercept different from zero but no trend:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6m 40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def compute_pair_metrics(security, candidates):\n",
    "    # `security` is a Series (one asset's prices over time).\n",
    "    security = security.div(security.iloc[0])\n",
    "    # `ticker` extracts the name (e.g., ETF ticker) for later use in results.\n",
    "    ticker = security.name \n",
    "    # `candidates` is a DataFrame (multiple assets' prices, same time index).\n",
    "    candidates = candidates.div(candidates.iloc[0])\n",
    "    spreads = candidates.sub(security, axis=0)\n",
    "    n, m = spreads.shape\n",
    "    X = np.ones(shape=(n, 2))\n",
    "    X[:, 1] = np.arange(1, n + 1)\n",
    "    \n",
    "    # Compute drift (vectorized)\n",
    "    # What it does: Performs vectorized OLS regression of each spread column on the time trend (X). \n",
    "    # Computes the coefficient matrix using the normal equation: β = (X'X)^(-1) X'y, where y is spreads. \n",
    "    # Takes the second row (iloc[1]) as the slope (drift), converts to a DataFrame column named 'drift'.\n",
    "    # Why?: Drift measures how much the spread trends over time (e.g., positive drift means widening). \n",
    "    # Low absolute drift proxies for a stable, mean-reverting relationship (heuristic for cointegration).\n",
    "    # Details: @ is matrix multiplication. inv computes inverse. This is efficient for all m candidates \n",
    "    # at once (vectorized over columns). Index matches candidate tickers.\n",
    "    # Context: Markdown notes low drift as a proxy; it's part of screening to reduce candidates before expensive tests.\n",
    "    drift = ((np.linalg.inv(X.T @ X) @ X.T @ spreads).iloc[1].to_frame('drift'))\n",
    "    \n",
    "    # Compute volatility (vectorized)\n",
    "    # What it does: Computes the standard deviation of each spread column, as a Series converted to DataFrame 'vol'.\n",
    "    # Why?: Volatility of the spread indicates trading opportunities—higher variance means more frequent/wider divergences for profits, \n",
    "    # but too high might signal instability. Low vol (with low drift) heuristics for cointegration.\n",
    "    # Details: std() defaults to sample std (ddof=1). Index: candidate tickers.\n",
    "    # Context: Empirical studies (e.g., Huck and Afawubo 2015) show cointegrated pairs have ~2x higher spread volatility\n",
    "    # than distance-based pairs, making this a key screener.\n",
    "    vol = spreads.std().to_frame('vol')\n",
    "    \n",
    "    # Return correlation (vectorized)\n",
    "    # What it does: Computes percentage changes (returns) for candidates and security, \n",
    "    # then pairwise correlations of each candidate's returns with the security's returns.\n",
    "    # Why?: High return correlation indicates comovement on a daily basis, a distance heuristic. \n",
    "    # But as markdown notes, correlation ≠ cointegration (e.g., correlated trends without mean-reversion).\n",
    "    # Details: pct_change() is (price_t / price_{t-1} - 1). corrwith uses Pearson correlation. Handles NaNs from first row.\n",
    "    corr_ret = (candidates.pct_change().corrwith(security.pct_change()).to_frame('corr_ret'))\n",
    "    \n",
    "    # Normalized price series correlation (vectorized)\n",
    "    corr = candidates.corrwith(security).to_frame('corr')\n",
    "    metrics = drift.join(vol).join(corr).join(corr_ret).assign(n=n)\n",
    "    \n",
    "    # Parallelize the slow per-candidate tests\n",
    "    def process_candidate(candidate, prices):\n",
    "        df = pd.DataFrame({'s1': security, 's2': prices})\n",
    "        var = VAR(df.values)\n",
    "        lags = var.select_order()  # This is still per-pair; consider fixing if too slow\n",
    "        k_ar_diff = lags.selected_orders['aic']\n",
    "        cj0 = coint_johansen(df, det_order=0, k_ar_diff=k_ar_diff)\n",
    "        t1, p1 = coint(security, prices, trend='c')[:2]\n",
    "        t2, p2 = coint(prices, security, trend='c')[:2]\n",
    "        return [ticker, candidate, t1, p1, t2, p2, k_ar_diff, *cj0.lr1]\n",
    "    \n",
    "    # Run in parallel (use n_jobs=8-12 if all cores overheat; -1 uses all)\n",
    "    tests = Parallel(n_jobs=-1)(\n",
    "        delayed(process_candidate)(candidate, prices) \n",
    "        for candidate, prices in candidates.items()\n",
    "    )\n",
    "    \n",
    "    columns = ['s1', 's2', 't1', 'p1', 't2', 'p2', 'k_ar_diff', 'trace0', 'trace1']\n",
    "    tests = pd.DataFrame(tests, columns=columns).set_index('s2')\n",
    "    return metrics.join(tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer loop remains the same, but now inner is parallelized\n",
    "spreads = []\n",
    "start = 2010\n",
    "stop = 2019\n",
    "etf_candidates = etfs.loc[str(start): str(stop), :]\n",
    "stock_candidates = stocks.loc[str(start): str(stop), :]\n",
    "s = time.time()\n",
    "for i, (etf_ticker, etf_prices) in enumerate(etf_candidates.items(), 1):\n",
    "    df = compute_pair_metrics(etf_prices, stock_candidates)\n",
    "    spreads.append(df.set_index('s1', append=True))\n",
    "    if i % 10 == 0:\n",
    "        print(f'\\n{i:>3} {time.time() - s:.1f}\\n')\n",
    "        s = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:51.082000Z",
     "start_time": "2021-02-23T15:13:51.071380Z"
    }
   },
   "outputs": [],
   "source": [
    "# def compute_pair_metrics(security, candidates):\n",
    "#     security = security.div(security.iloc[0])\n",
    "#     ticker = security.name\n",
    "#     candidates = candidates.div(candidates.iloc[0])\n",
    "#     spreads = candidates.sub(security, axis=0)\n",
    "#     n, m = spreads.shape\n",
    "#     X = np.ones(shape=(n, 2))\n",
    "#     X[:, 1] = np.arange(1, n + 1)\n",
    "    \n",
    "#     # compute drift\n",
    "#     drift = ((np.linalg.inv(X.T @ X) @ X.T @ spreads).iloc[1]\n",
    "#              .to_frame('drift'))\n",
    "    \n",
    "#     # compute volatility\n",
    "#     vol = spreads.std().to_frame('vol')\n",
    "    \n",
    "#     # return correlation\n",
    "#     corr_ret = (candidates.pct_change()\n",
    "#                 .corrwith(security.pct_change())\n",
    "#                 .to_frame('corr_ret'))\n",
    "    \n",
    "#     # normalized price series correlation\n",
    "#     corr = candidates.corrwith(security).to_frame('corr')\n",
    "#     metrics = drift.join(vol).join(corr).join(corr_ret).assign(n=n)\n",
    "    \n",
    "#     tests = []\n",
    "#     # run cointegration tests\n",
    "#     for candidate, prices in tqdm(candidates.items()):\n",
    "#         df = pd.DataFrame({'s1': security, 's2': prices})\n",
    "#         var = VAR(df.values)\n",
    "#         lags = var.select_order() # select VAR order\n",
    "#         k_ar_diff = lags.selected_orders['aic']\n",
    "#         # Johansen Test with constant Term and estd. lag order\n",
    "#         cj0 = coint_johansen(df, det_order=0, k_ar_diff=k_ar_diff)\n",
    "#         # Engle-Granger Tests\n",
    "#         t1, p1 = coint(security, prices, trend='c')[:2]\n",
    "#         t2, p2 = coint(prices, security, trend='c')[:2]\n",
    "#         tests.append([ticker, candidate, t1, p1, t2, p2, \n",
    "#                       k_ar_diff, *cj0.lr1])\n",
    "#     columns = ['s1', 's2', 't1', 'p1', 't2', 'p2', 'k_ar_diff', 'trace0', 'trace1']\n",
    "#     tests = pd.DataFrame(tests, columns=columns).set_index('s2')\n",
    "#     return metrics.join(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:44:09.355159Z",
     "start_time": "2021-02-23T15:16:00.355882Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# spreads = []\n",
    "# start = 2010\n",
    "# stop = 2019\n",
    "# etf_candidates = etfs.loc[str(start): str(stop), :]\n",
    "# stock_candidates = stocks.loc[str(start): str(stop), :]\n",
    "# s = time()\n",
    "# for i, (etf_ticker, etf_prices) in enumerate(etf_candidates.items(), 1):\n",
    "#     df = compute_pair_metrics(etf_prices, stock_candidates)\n",
    "#     spreads.append(df.set_index('s1', append=True))\n",
    "#     if i % 10 == 0:\n",
    "#         print(f'\\n{i:>3} {time() - s:.1f}\\n')\n",
    "#         s = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.577216Z",
     "start_time": "2021-02-23T15:01:38.727Z"
    }
   },
   "outputs": [],
   "source": [
    "names = get_ticker_dict()\n",
    "spreads = pd.concat(spreads)\n",
    "spreads.index.names = ['s2', 's1']\n",
    "spreads = spreads.swaplevel()\n",
    "spreads['name1'] = spreads.index.get_level_values('s1').map(names)\n",
    "spreads['name2'] = spreads.index.get_level_values('s2').map(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.578888Z",
     "start_time": "2021-02-23T15:01:38.730Z"
    }
   },
   "outputs": [],
   "source": [
    "spreads['t'] = spreads[['t1', 't2']].min(axis=1)\n",
    "spreads['p'] = spreads[['p1', 'p2']].min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engle-Granger vs Johansen: how do their findings compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for the significance of the cointegration tests, we compare the Johansen trace statistic for rank 0 and 1 to their respective critical values and obtain the Engle-Granger p-value.\n",
    "\n",
    "We follow the recommendation by Gonzalo and Lee (1998) to apply both tests and accept pairs where they agree. The authors suggest additional due diligence in case of disagreement, which we are going to skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.580513Z",
     "start_time": "2021-02-23T15:01:38.733Z"
    }
   },
   "outputs": [],
   "source": [
    "spreads['trace_sig'] = ((spreads.trace0 > trace0_cv) &\n",
    "                        (spreads.trace1 > trace1_cv)).astype(int)\n",
    "spreads['eg_sig'] = (spreads.p < .05).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the over 46,000 pairs across both sample periods, the Johansen test considers 3.2 percent of the relationships as significant, while the Engle-Granger considers 6.5 percent. They agree on 366 pairs (results may change with new data downloaded from stooq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.582004Z",
     "start_time": "2021-02-23T15:01:38.736Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(spreads.eg_sig, spreads.trace_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.583606Z",
     "start_time": "2021-02-23T15:01:38.739Z"
    }
   },
   "outputs": [],
   "source": [
    "spreads['coint'] = (spreads.trace_sig & spreads.eg_sig).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.585193Z",
     "start_time": "2021-02-23T15:01:38.741Z"
    }
   },
   "outputs": [],
   "source": [
    "spreads.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.587625Z",
     "start_time": "2021-02-23T15:01:38.744Z"
    }
   },
   "outputs": [],
   "source": [
    "spreads = spreads.reset_index()\n",
    "spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.588996Z",
     "start_time": "2021-02-23T15:01:38.746Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=np.log1p(spreads.t.abs()), \n",
    "                y=np.log1p(spreads.trace1), \n",
    "                hue='coint', data=spreads[spreads.trace0>trace0_cv]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.589724Z",
     "start_time": "2021-02-23T15:01:38.749Z"
    }
   },
   "outputs": [],
   "source": [
    "spreads.to_hdf('heuristics.h5', 'spreads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.590297Z",
     "start_time": "2021-02-23T15:01:38.751Z"
    }
   },
   "outputs": [],
   "source": [
    "spreads = pd.read_hdf('heuristics.h5', 'spreads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.590796Z",
     "start_time": "2021-02-23T15:01:38.755Z"
    }
   },
   "outputs": [],
   "source": [
    "spreads.drift = spreads.drift.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.591326Z",
     "start_time": "2021-02-23T15:01:38.757Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(spreads.eg_sig, spreads.trace_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.591967Z",
     "start_time": "2021-02-23T15:01:38.760Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: f'{x:.2%}')\n",
    "pd.crosstab(spreads.eg_sig, spreads.trace_sig, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.592458Z",
     "start_time": "2021-02-23T15:01:38.762Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, figsize=(20, 5))\n",
    "for i, heuristic in enumerate(['drift', 'vol', 'corr', 'corr_ret']):\n",
    "    sns.boxplot(x='coint', y=heuristic, data=spreads, ax=axes[i])\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well do the heuristics predict significant cointegration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare the distributions of the heuristics for series that are cointegrated according to both tests with the remainder that is not, volatility and drift are indeed lower (in absolute terms). Figure 9.14 shows that the picture is less clear for the two correlation measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.592973Z",
     "start_time": "2021-02-23T15:01:38.765Z"
    }
   },
   "outputs": [],
   "source": [
    "spreads.groupby(spreads.coint)[['drift', 'vol', 'corr']].describe().stack(level=0).swaplevel().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.593502Z",
     "start_time": "2021-02-23T15:01:38.770Z"
    }
   },
   "outputs": [],
   "source": [
    "spreads.coint.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the predictive accuracy of the heuristics, we first run a logistic regression model with these features to predict significant cointegration. It achieves an area-under-the-curve (AUC) cross-validation score of 0.815; excluding the correlation metrics, it still scores 0.804. A decision tree does slightly better at AUC=0.821, with or without the correlation features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.594154Z",
     "start_time": "2021-02-23T15:01:38.777Z"
    }
   },
   "outputs": [],
   "source": [
    "y = spreads.coint\n",
    "X = spreads[['drift', 'vol', 'corr', 'corr_ret']]\n",
    "# X = spreads[['drift', 'vol']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.594658Z",
     "start_time": "2021-02-23T15:01:38.782Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.595242Z",
     "start_time": "2021-02-23T15:01:38.787Z"
    }
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegressionCV(Cs=np.logspace(-10, 10, 21), \n",
    "                               class_weight='balanced',\n",
    "                               scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.595833Z",
     "start_time": "2021-02-23T15:01:38.792Z"
    }
   },
   "outputs": [],
   "source": [
    "log_reg.fit(X=X, y=y)\n",
    "Cs = log_reg.Cs_\n",
    "scores = pd.DataFrame(log_reg.scores_[True], columns=Cs).mean()\n",
    "scores.plot(logx=True);\n",
    "f'C:{np.log10(scores.idxmax()):.2f}, AUC: {scores.max():.2%}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.596334Z",
     "start_time": "2021-02-23T15:01:38.797Z"
    }
   },
   "outputs": [],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.596850Z",
     "start_time": "2021-02-23T15:01:38.801Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = log_reg.predict_proba(X)[:, 1]\n",
    "confusion_matrix(y_true=spreads.coint, y_pred=(y_pred>.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.597418Z",
     "start_time": "2021-02-23T15:01:38.807Z"
    }
   },
   "outputs": [],
   "source": [
    "spreads.assign(y_pred=log_reg.predict_proba(X)[:, 1]).groupby(spreads.coint).y_pred.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not least due to the strong class imbalance, there are large numbers of false positives:\n",
    "correctly identifying 80 percent of the 366 cointegrated pairs implies over 16,500 false positives, but eliminates almost 30,000 of the candidates. See the notebook cointegration_\n",
    "tests for additional detail.\n",
    "\n",
    "The **key takeaway** is that distance heuristics can help screen a large universe more  efficiently, but this comes at a cost of missing some cointegrated pairs and still requires\n",
    "substantial testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.597962Z",
     "start_time": "2021-02-23T15:01:38.813Z"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(class_weight='balanced')\n",
    "decision_tree = GridSearchCV(model,\n",
    "                             param_grid={'max_depth': list(range(1, 10))},\n",
    "                             cv=5,\n",
    "                             scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.598628Z",
     "start_time": "2021-02-23T15:01:38.817Z"
    }
   },
   "outputs": [],
   "source": [
    "decision_tree.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.599171Z",
     "start_time": "2021-02-23T15:01:38.822Z"
    }
   },
   "outputs": [],
   "source": [
    "f'{decision_tree.best_score_:.2%}, Depth: {decision_tree.best_params_[\"max_depth\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.599733Z",
     "start_time": "2021-02-23T15:01:38.828Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(data=decision_tree.best_estimator_.feature_importances_, \n",
    "          index=X.columns).sort_values().plot.barh(title='Feature Importance')\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.600289Z",
     "start_time": "2021-02-23T15:01:38.832Z"
    }
   },
   "outputs": [],
   "source": [
    "spreads.assign(y_pred=decision_tree.predict_proba(X)[:, 1]).groupby(spreads.coint).y_pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:13:20.600835Z",
     "start_time": "2021-02-23T15:01:38.838Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(x='coint', \n",
    "            y='y_pred', \n",
    "            data=spreads.assign(y_pred=decision_tree.predict_proba(X)[:, 1]), \n",
    "            kind='box');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "309px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
